---
title: "Práctica 2"
author: "Lorena Romero y María Soto"
date: "2023-11-22"
output:
  
  html_document:
    df_print: paged
    highlight: kate
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: yes
---

```{r, eval=FALSE}
library(ggplot2)
library(tidyverse)
library(gridExtra)
library(reshape2)
library(GGally)
library(ggfortify)
library(caret)
library(dplyr)
library(FactoMineR)
library(factoextra)
library(plotly)
library(mlbench)
library(rpart.plot)
library(rnaturalearth)
library(purrr)
library(cowplot)
library(Metrics)
library(keras)
library(tensorflow)
```

```{r, include=FALSE}
library(ggplot2)
library(tidyverse)
library(gridExtra)
library(reshape2)
library(GGally)
library(ggfortify)
library(caret)
library(dplyr)
library(FactoMineR)
library(factoextra)
library(plotly)
library(mlbench)
library(rpart.plot)
library(rnaturalearth)
library(purrr)
library(cowplot)
library(Metrics)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
colorspie <- c("skyblue", "plum3", "pink2", "sandybrown", "palegreen3")
colorpieborde <- c("skyblue4", "plum4", "pink3", "salmon3", "palegreen4")
```

# Apartado A
## Descripción del conjunto de datos
**Describe brevemente el conjunto de entrenamiento.**

Vamos a trabajar sobre un conjunto de datos en el que se recogen las canciones más populares de Spotify a lo largo de un tiempo. Cada entrada recoge una amplia variedad de información de cada canción que analizaremos más adelante. Nuestro objetivo es utilizar estos datos para predecir la popularidad de canciones.

El conjunto de datos de entrenamiento consta de las siguientes variables categóricas:
`spotify_id`, `name`, `artists`, `country`, `is_explicit`,  `album_name`, `key`, `mode`, `time_signature`

Y, por otra parte, las siguientes variables numéricas:
`daily_rank`, `daily_movement`, `weekly_movement`, `snapshot_date`, `duration_ms`, `danceability`, `energy`, `loudness`, `speechiness`, `accousticness`,`liveness`, `instrumentalness` , `valence`, `tempo`, `popularity` y `album_release_date`

```{r}
spotify <- read.csv("spoti.csv", na.strings = "")

summary(spotify)
```
Sin embargo, aquí ya vemos que hay algunas variables que se toman como numéricas cuando hemos hablado de ellas como categóricas, y las fechas que son leídas como strings. Este problema lo trataremos después. 

Pero para hacer un pequeño análisis haremos un nuevo dataset donde no tendremos en cuenta los países. Después, haremos la media de popularidad de cada canción para ver cómo de populares son las canciones "mundialmente".

```{r}
spotify1 = spotify
#eliminamos columna paises
spotify1 = spotify1[,-c(1,8)]

# Calcula la media de la popularidad para cada canción
mean_popularity <- aggregate(popularity ~ name, data = spotify1, mean)

categories <- cut(spotify1$popularity, breaks = seq(0, 100, by = 10))
barplot(table(categories), main = "Recuento de Canciones por Intervalo de Popularidad", xlab = "Intervalo de Popularidad", ylab = "Recuento", col = colorspie)


```

Viendo este gráfico se nos ocurre para hacer nuestro pequeño análisis 4 grupos principales.

- **Nada populares**, cuyo rango será de 0 a 30, este será el menor grupo ya que como se observa en la gráfica no hay prácticamente canciones que hayan sido tan poco populares.

- **Poco populares**, cuyo rango de popularidad será de 30 a 60, este rango abarca canciones que han sido conocidas pero tampoco mucho.

- **Algo populares**, cuyo rango de popularidad sera de 60 a 90, son canciones que sí han sido populares.

- **Hit**, son las canciones que su popularidad está entre 90 y 100, es decir, que han sido las más populares y muchísima gente las conoce.

Después de haber hecho esta división vamos a seguir haciendo este análisis sin tener en cuenta los países, sino la popularidad "mundial" tras haber hecho la media para no ver en el análisis tantos valores donde la mayoría van a ser repetidos. Para ello ahora vamos a empezar visualizando las variables categóricas que pueden tener algo de relación con `popularity`.

Vamos a utilizar `analysis_data`, donde en principio no vamos a tener en cuenta el país:
```{r}
#primero convertimos las fechas
spotify1$snapshot_date = as.Date(spotify1$snapshot_date)
spotify1$album_release_date = as.Date(spotify1$album_release_date)

#creamos analysis_data donde hacemos media de todas las variables, ya que si estas tienen algun remix por ejemplo
#cambiaran su valencia, energia pero por muy pocas centesimas.
analysis_data <- aggregate(cbind(popularity, daily_rank, weekly_movement,daily_movement, danceability, energy, loudness, key, speechiness, acousticness, instrumentalness, liveness, valence, tempo, time_signature, duration_ms) ~ name + artists + album_name + is_explicit + album_release_date + mode, data = spotify1, mean)

#ponemos las fechas como minimo, ya que nos interesa cuando entraron a la lista por primera vez
analysis_data2 <- aggregate(cbind(snapshot_date) ~  name + artists + album_name + is_explicit + album_release_date + mode , data = spotify1, min)

analysis_data$snapshot_date = analysis_data2$snapshot_date

analysis_data$popularity_group <- cut(analysis_data$popularity, breaks = c(0, 30, 60, 90, 101), labels = c("Nada popular", "Poco popular", "Algo popular", "Hit"),include.lowest = TRUE)
analysis_data$popularity_group <- factor(analysis_data$popularity_group, levels = c("Nada popular", "Poco popular", "Algo popular", "Hit"))


```

- Variable `key`
Primero la pasamos a factor ya que actualmente se interpreta como numérica.
```{r}
analysis_data$key = factor(analysis_data$key)
myplot <- ggplot(data = analysis_data, aes(x = key, fill = popularity_group, color = popularity_group)) +
  geom_bar(alpha = 0.8, position = "dodge") +
  ggtitle("Relación entre la clave y la popularidad") +
  labs(x = "Key", y = "", fill = "Popularity Group") +
  scale_color_manual(values = colorpieborde, name = "Popularity Group") +
  scale_fill_manual(values = colorspie, name = "Popularity Group")
myplot

table(analysis_data$popularity_group, analysis_data$key)
```

Podemos ver como las canciones más populares son las que están compuestas en key 1. Esto nos hace pensar que esta variable si que estará relacionada con la popularidad debido a que puede aportar algo a las canciones que las haga más atractivas a pesar de que seguramente la persona que escucha la música no sabe la clave en la que está compuesta.

- `mode`

Analizando esta variable hemos descubierto que hay canciones que tienen un cambio de modo, es decir, que si una canción está en un principio en una tonalidad mayor otra variación de esta, un remix o incluso un error ocasiona que en algunas canciones haya cambios de modo. Entonces, si hacemos la media de esta variable agrupando las canciones por nombre y el resto de variables hay alguna cuyo modo varía, dando así valores intermedios, por lo que en el agrupamiento anteriormente hemos decidido que no se haga la media de esta variable.

```{r}
analysis_data$mode = factor(analysis_data$mode)
myplot <- ggplot(data = analysis_data, aes(x = mode, fill = popularity_group, color = popularity_group)) +
  geom_bar(alpha = 0.8, position = "dodge") +
  ggtitle("Relación entre el modo y la popularidad") +
  labs(x = "Mode", y = "", fill = "Popularity Group") +
  scale_color_manual(values = colorpieborde, name = "Popularity Group") +
  scale_fill_manual(values = colorspie, name = "Popularity Group")
myplot

table(analysis_data$popularity_group, analysis_data$mode)
```

En esta gráfica no destaca mucho la diferencia de datos entre los modos aunque para los grupos da canciones nada populares, algo populares y hit hay más cantidad en modo 1 por lo que tal vez hay cierta relación pero no lo tenemos del todo claro.


- `is_explicit`

```{r}
analysis_data$is_explicit = factor(analysis_data$is_explicit)
myplot <- ggplot(data = analysis_data, aes(x = is_explicit, fill = popularity_group, color = popularity_group)) +
  geom_bar(alpha = 0.8, position = "dodge") +
  ggtitle("Relación entre el modo y la popularidad") +
  labs(x = "Explicita", y = "", fill = "Popularity Group") +
  scale_color_manual(values = colorpieborde, name = "Popularity Group") +
  scale_fill_manual(values = colorspie, name = "Popularity Group")
myplot

table(analysis_data$popularity_group, analysis_data$is_explicit)
```
La mayoría de canciones no explícitas son más populares que las canciones explícitas, por lo que quizás esto también juega un papel importante en la popularidad.

- `time_signature`
```{r}
myplot = ggplot(data=analysis_data, aes(x=time_signature, fill=popularity_group, color = popularity_group)) +
  geom_bar(alpha=0.8, position="dodge")+
  labs(x = "Time Signature",y="") +
  scale_color_manual(values = colorpieborde, name="popularity")+
  scale_fill_manual(values = colorspie, name="popularity")
myplot
```
Vemos como casi todas las canciones tienen un `time_signature` de 4, y que no hay ninguna con 2. Por lo tanto seguramente esta variable sea un poco indiferente a la hora de conocer la popularidad. Pero nos ayudará a determinar que la mayoría de canciones se hacen populares cuando su `time_signature` es de 4.

- `name`, `artists`, `album_name` 
Aunque estas variables no van a jugar un papel fundamental en nuestro modelo, es más, vamos a prescindir de ellas ya que lo que pretendemos es saber si una canción se hará o no famosa debido a sus atributos intrínsecos. Vamos a mostrar las canciones más populares, los artistas, y el nombre del album

```{r}
analysis_datasorted = analysis_data[order(-analysis_data$popularity),]

N<-10
artistastop = analysis_datasorted$artists[1:10]
cat("Artistas más populares\n")
artistastop
```
Estos son el top de artistas con más popularidad en este momento. Cada uno tiene diferentes canciones populares:

```{r}
popularity_counts <- table(analysis_datasorted$artists)

cat("Número de canciones en el dataset para cada artista:\n")
for (artist in artistastop) {
  cat(artist, ":", popularity_counts[artist], "canciones en la lista\n")
}
```
Y estas son las canciones y el nombre del álbum:
```{r}
canciones <- data.frame(
  Artista = analysis_datasorted$artists[1:10],
  Nombre = analysis_datasorted$name[1:10],
  Álbum = analysis_datasorted$album_name[1:10]
)
canciones
```
Esto no coincide con el ranking y somos conscientes de ello pero todavía no hemos analizado esas partes. Como recordatorio, esto solo lo hemos hecho para hablar un poco de los datos, estas tres variables no las vamos a tener muy en cuenta de ahora en adelante.

***Variables continuas***

- `daily_rank`

```{r}
analysis_data$daily_rank = round(analysis_data$daily_rank)

analysis_data$daily_rank_group <- cut(analysis_data$daily_rank, breaks = c(0, 10, 20, 30, 40, 50), labels = c("Top 10", "Top 20", "Top 30", "Top 40", "Top 50"),include.lowest = TRUE)

myplot = ggplot(data=analysis_data, aes(x=daily_rank_group, fill=popularity_group, color = popularity_group)) +
  geom_bar(alpha=0.8, position="dodge")+
  labs(x = "Daily_Rank",y="") +
  scale_color_manual(values = colorpieborde, name="popularity")+
  scale_fill_manual(values = colorspie, name="popularity")
myplot
```
Esto nos hace pensar que el ranking actual no significa que una canción sea muy popular. Estas van a ir ganando popularidad quizás con el tiempo que permanezca en la lista, pero las 10 primeras no se acercan en absoluto a las canciones más populares. 

- `daily_movement`

```{r}
analysis_data$daily_movement = round(analysis_data$daily_movement)

analysis_data$daily_movement_group <- cut(analysis_data$daily_movement, breaks = c(-10, 0, 5, 10, 20, 30, 40, 50), labels = c("Baja puestos", "Se mantiene", "Sube puestos", "Sube bastantes puestos", "Sube muchos", "Se hace famosísima de repente", "Da un boom"),include.lowest = TRUE)

myplot = ggplot(data=analysis_data, aes(x=daily_movement_group, fill=popularity_group, color = popularity_group)) +
  geom_bar(alpha=0.8, position="dodge")+
  labs(x = "Daily Movement",y="") +
  scale_color_manual(values = colorpieborde, name="popularity")+
  scale_fill_manual(values = colorspie, name="popularity")+
  theme(axis.text.x = element_text(angle = 15, hjust = 1))
myplot
```
El movimiento diario nos muestra que las canciones que se mantienen son las más populares, esto es seguramente por aguantar más tiempo en un ranking superior.

- `weekly_movement`

```{r}
analysis_data$weekly_movement = round(analysis_data$weekly_movement)

analysis_data$weekly_movement_group <- cut(analysis_data$weekly_movement, breaks = c(-25, 0, 5, 10, 20, 30, 40, 80), labels = c("Baja puestos", "Se mantiene", "Sube puestos", "Sube bastantes puestos", "Sube muchos", "Se hace famosísima de repente", "Da un boom"),include.lowest = TRUE)

myplot = ggplot(data=analysis_data, aes(x=weekly_movement_group, fill=popularity_group, color = popularity_group)) +
  geom_bar(alpha=0.8, position="dodge")+
  labs(x = "Weekly Movemente",y="") +
  scale_color_manual(values = colorpieborde, name="popularity")+
  scale_fill_manual(values = colorspie, name="popularity")+
  theme(axis.text.x = element_text(angle = 15, hjust = 1))
myplot
```
Podemos ver como al igual que antes son las canciones que se mantienen las que más populares son.


- `snapshot_date` interesante análisis junto `album_realease_date`

Creemos que es interesante analizar el tiempo que tarda una canción en entrar en el dataset porque va a empezar a ganar popularidad para ello vamos a hacer una columna con el número de días que una canción tarda en hacerse famosa

```{r}
analysis_data$snapshot_date = as.Date(analysis_data$snapshot_date)
analysis_data$album_release_date = as.Date(analysis_data$album_release_date)

diferencia_dias = as.numeric(analysis_data$snapshot_date - analysis_data$album_release_date)
analysis_data$numDiasFama = diferencia_dias

analysis_data$numDiasFama = ifelse(analysis_data$numDiasFama <= 0, 0, analysis_data$numDiasFama)

analysis_data$numDiasFama_group <- cut(analysis_data$numDiasFama, breaks = c(-1, 1, 10, 30, 60, 90, 120, 99999), labels = c("Entra al salir", "10 dias", "1 mes", "2 meses", "3 meses", "4 meses", "+"),include.lowest = TRUE)
sum(is.na(analysis_data$numDiasFama))
sum(is.na(analysis_data$numDiasFama_group))
myplot = ggplot(data=analysis_data, aes(x=numDiasFama_group, fill=popularity_group, color = popularity_group)) +
  geom_bar(alpha=0.8, position="dodge")+
  labs(x = "Número Días Hasta Dataset",y="") +
  scale_color_manual(values = colorpieborde, name="popularity")+
  scale_fill_manual(values = colorspie, name="popularity")
myplot
```
Son muy pocas las canciones que nada más salir entran al dataset de spotify. La mayoría de canciones que entran tienen mucha antigüedad siendo gran parte bastante populares. Además, cabe destacar que la popularidad de las que entran a los 10 dias de salir es baja.


- **CARACTERÍSTICAS DE LAS CANCIONES**
`duration_ms`, `danceability`, `energy`, `loudness`, `speechiness`, `accousticness`, `liveness`, `instrumentalness`, `valence`, `tempo`, `time_signature`

```{r}
analysisContinuas= analysis_data[ , c("popularity_group", "duration_ms", "danceability","energy", "loudness", "speechiness", "acousticness", "liveness", "instrumentalness", "valence", "tempo")]
mymelt=melt(analysisContinuas, id.vars=1, value.name="FeatureValue", variable.name="Feature")

myplot = ggplot(data=mymelt, aes(x=FeatureValue, fill=popularity_group, color = popularity_group)) +
  geom_density(alpha=0.8)+
  ggtitle("Relación entre las variables continuas y la variable de clase")+
  labs(x = "",y="", fill = "") +
  scale_color_manual(values = colorpieborde, name="")+
  scale_fill_manual(values = colorspie, name="")+
  facet_wrap(~ Feature, ncol=3, scales = "free")
myplot
```
Nos sorprende ver que todas estas variables parecen tener una gran correlación con la variable de clase. Ya que podemos verlas todas superpuestas. Sin embargo, vemos que hay valores, en general, muy dispersos y nos gustaría trabajar con ellos, quizá, en la misma escala; es por esto que más adelante seguramente se normalicen estos valores.

Creemos que además de con la popularidad, estas variables van a estar bastante correlacionadas entre sí.

```{r}
analysisContinuas= analysis_data[ , c("duration_ms", "danceability","energy", "loudness", "speechiness", "acousticness", "liveness", "instrumentalness", "valence", "tempo")]
corrplot::corrplot(cor(analysisContinuas))

```
Y tal como imaginamos podemos ver la relación que existe entre estas variables. Por ejemplo la relación negativa que hay entre `acousticness` y `energy` que puede ser debido a que el acústico transmite menos energía. O que las canciones donde hay energía sean más "ruidosas". Nos parece muy interesante que, en general, la mayoría están relacionadas.

Antes de comenzar con las preguntas del apartado 1, nos gustaría hacer un pequeño PCA para mostrar si con las variables que tenemos numéricas seríamos capaces de determinar algo de información sobre la popularidad.

```{r, eval=FALSE}
analysis_pca = analysisContinuas
analysis_pca$daily_rank = analysis_data$daily_rank
analysis_pca$weekly_movement = analysis_data$weekly_movement 
analysis_pca$daily_movement = analysis_data$daily_movement
analysis_pca$time_signature = analysis_data$time_signature
analysis_pca$numDiasFama = analysis_data$numDiasFama
analysis_pca_estatus = analysis_data$popularity_group

# PCA
pca_result <- prcomp(analysis_pca, scale = TRUE)
row.names(pca_result$x) = analysis_pca_estatus
pcas = as.data.frame(pca_result$x, stringsAsFactors =F)
pcas = cbind(ESTATUS = analysis_pca_estatus,pcas)
```

```{r, eval=FALSE}
ggplot(pcas, aes(PC1, PC2, color = ESTATUS, shape=ESTATUS)) +
  geom_point(size = 1, alpha=0.6) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  scale_color_manual(values = colorspie) +
  scale_shape_manual(values = c(17, 16, 15, 14)) +
  xlab("Primera componente principal") +
  ylab("Segunda componente principal") +
  ggtitle("Las dos primeras componentes principales de titanicData") +
  guides(color = guide_legend(title = "Estado de Popularidad"), shape = guide_legend(title = "Estado de Popularidad")) +
  theme_minimal() +
  theme(legend.position = "top")
```

```{r,eval=FALSE}
autoplot(pca_result, data=analysis_data, cex = 0.7, alpha=0.6, color = "popularity_group",
         loadings = TRUE, loadings.colour = 'gold',
         loadings.label = TRUE, loadings.label.size = 4, loadings.label.colour = 'mediumvioletred')

fviz_pca_var(pca_result, col.var = "contrib",
             gradient.cols = c("white", "blue", "red"), 
             repel = TRUE)
```

Debido a la gran cantidad de datos es casi imposible ver lo que nos muestra este PCA, aunque podemos ver la parte representativa de cada componente en el análisis. También se nos dice que variables como weekly_movement, energy, loudness... Presentan una gran variación para nuestros datos.

```{r, eval=FALSE}
(VE <- pca_result$sdev^2)
PVE <- VE / sum(VE)
cumsum(round(PVE, 2))
```

Al realizar un análisis de componentes principales (PCA), obsevamos que al utilizar únicamente 11 variables numéricas de las 26 disponibles, se logra explicar el 91% de la varianza total. Estos 11 componentes principales tienen valores propios que indican la cantidad de variabilidad que cada uno explica. Además, al observar la acumulación de porcentajes de varianza explicada, notamos que con estas 11 variables ya se captura una proporción significativa de la complejidad de los datos. Esta eficiencia en la reducción de dimensionalidad sugiere que, posiblemente, no sea necesario utilizar todas las variables originales para construir un modelo efectivo.

Finalmente vamos a analizar un poco de la variable `country` la que hemos eliminado anteriormente sin tener reparo en que esta variable tendrá canciones con popularidad distinta en cada país, vamos a quedarnos con aquellas con mayor popularidad y por lo tanto vamos a ver solo un poco de como afectan las variables numéricas a los distintos países.
*TODO*

```{r, eval=FALSE}
analysis_data <- aggregate(cbind(popularity, daily_rank, weekly_movement,daily_movement, danceability, energy, loudness, key, speechiness, acousticness, instrumentalness, liveness, valence, tempo, time_signature, duration_ms) ~ name + artists + album_name + is_explicit + album_release_date + mode + country, data = spotify, mean)

analysis_data2 <- aggregate(cbind(snapshot_date) ~  name + artists + album_name + is_explicit + album_release_date + mode + country , data = spotify, min)

analysis_data$snapshot_date = analysis_data2$snapshot_date

# Cargar datos de límites de países
world <- ne_countries(returnclass = "sf")

world = world[, c("iso_a2", "geometry")]

# Crear un conjunto de datos de ejemplo
spotify_mean <- data.frame(
  country= analysis_data$country,
  popularity = analysis_data$popularity,
  is_explicit = analysis_data$is_explicit,
  duration_ms = analysis_data$duration_ms, 
  danceability = analysis_data$danceability,
  energy = analysis_data$energy,
  loudness = analysis_data$loudness,
  speechiness = analysis_data$speechiness,
  liveness = analysis_data$liveness,  
  instrumentalness = analysis_data$instrumentalness,
  valence = analysis_data$valence, 
  tempo = analysis_data$tempo
)

# Quedarse con las filas en las que la popularidad es mayor que 80
spotify_mean <- spotify_mean[spotify_mean$popularity > 80, ]

# Unir datos de Spotify con datos del mundo
world_spotify <- merge(world, spotify_mean, by.x = "iso_a2", by.y = "country", all.x = FALSE)

# Lista de variables a usar en cada mapa
variables <- c("duration_ms", "danceability", "energy", "loudness", "speechiness", "liveness", "valence", "tempo")

# Crear una lista de gráficos usando purrr::map
maps <- map(variables, function(var) {
  ggplot() +
    geom_sf(data = world_spotify, aes_string(fill = var)) +
    scale_fill_gradient(low = "lightyellow", high = "sandybrown") +
    theme_minimal() +
    labs(title = var)  # Título con el nombre de la variable
})

plot_grid(plotlist = maps, ncol = 3)
```

Aquí podemos ver como la `danceability` de las canciones más populares sera distinta en cada país. Y otras características como por ejemplo la `valence` en Oceanía disfrutaran de canciones más calmadas que por ejemplo en sur América.

```{r}
rm(analysis_data, analysis_data2, analysis_datasorted,mean_popularity, canciones, mymelt, artist, pcas, pca_result, spotify_mean, spotify1, world, world_spotify, myplot, maps, analysisContinuas, analysis_pca, analysis_pca_estatus, artistastop, categories, diferencia_dias, N, popularity_counts, PVE, variables, VE)
```

Después de haber hecho este pequeño análisis, cuyos datos son una muestra representativa del conjunto inicial pasamos a responder las preguntas del apartado A.

**Describe además, los atributos predictores del conjunto y la variable a predecir. Divide la descripción de los atributos en cuatro grupos, a saber. Justifica el porqué entiendes que cada atributo tiene o no relación con `popularity`.**

**1. predictores numéricos que, a priori, no tienen relación alguna con `popularity` y por tanto no son útiles para su predicción.**

Vamos a nombrar cada variable que pensamos que no tienen relación con su correspondiente justificación.

- `album_release_date`: no estamos muy seguras de si esta variable se tratará más adelante o si tendrá relación con la popularidad.
- `snapshot_date`: La fecha en la que se recogieron los datos podría ser importante porque la popularidad de una canción puede variar con el tiempo aunque no estamos muy seguro de ellos.

**2. Predictores numéricos que pueden aportar algo a la predicción de `popularity` y predictores categóricos.**

Como hemos hecho en el apartado anterior, vamos ir explicanco junto a la variable que creemos que sí puede aportar el porqué de esta creencia.

Para los predictores numéricos:

- `daily_rank`: La posición diaria en el ranking puede ser un indicador importante de la popularidad de una canción. Si una canción ocupa constantemente un lugar alto es probable que sea popular.

- `daily_movement`: El cambio diario de la posición podría indicar tendencias en la popularidad de la canción, en caso de subir seguramente indique un aumento de popularidad.

- `weekly_movement`: Es parecido al anterior, pero a más largo plazo.

- `valence`: La media de positivismo transmitido por la canción podría influir en su atractivo general, las cancioens más alegres o las más triste suelen ser las más populares pero si no trasmites nada seguramente no sea popular.

- `tempo`: El número de beats por minuto puede influir en la percepción de energía de una canción y ser más popular.

- `energy`: La energía de la canción podría ser un factor calve en su atractivo puesto que las canciones más enérgicas podrían tener mayor probabilidad de ser populares.

- `speechiness`: La presencia de palabra hablada en una canción es muy importante pues la gran mayoría de canciones (por no decir todas) que están en los rankings de popularidad suelen ser todas con letra.

- `instrumentalness`: La predominandia de la instrumentación sobre la voz pordría influir de la misma forma que la anterior.

- `duration_ms`: como hemos visto antes esta variable podria estar relacionada con la popularidad

- `liveness`: el porcentaje de una canción que se grabó en vivo o en estudio influirá en la popularidad.

- `acousticness`: la calidad acústica de la música es algo que tambien tendra relación.

- `loudness`: pensamos que el nivel de decibelios al que fue grabada la canción importará para la popularidad.

- `danceability`: La capacidad de una canción para ser baildad podría ser una factor importante.


Y para los predictores categóricos:

- `mode`: Si la canción está en modo mayor o menos podría influir en la percepción emocional y, por tanto, en su popularidad.

- `is_explicit`: La presencia de lenguaje explícito podría atraet a ciertos públicos y afectar. Por ejemplo, muchas canciones muy populares latinoamericanas tienen un alto porcentaje de palabras sexuales y escenarios explícitos y, a veces, cuan más explícito es más repercusión tiene. Por otra parte, canciones con un lenguaje explícito de violencia no suelen ser populares por motivos obvios.

- `key`: pensamos que la escala en la que está compuesta podrá afectar a la popularidad.

- `time_signature`: Creemos que el nivel en una esacala de ritmo musical tiene relación con las dos anteriores por lo que pensamos que puede ser relevante para la popularidad de una canción.

**3. Relacionados con `popularity`.**

Pensamos que las variables que más relación tienen con la popularidad son las características intrínsecas de la canción. Y que las variables que tienen que ver con los puestos que tiene una canción diariamente y semanal y sus movimientos entre los puestos. `daily_rank`, `daily_movement` y `weekly_movement` serán de gran utilidad para aprovechar la secuencialidad de los datos.


**4. No relacionados.**

- `artists`: El artista podría ser un factor importante pues artitas más reconocidos tienen más propabilidad de producir canciones populares. Pero, no nos servira en nuestro estudio ya que estamos planteando hacer un reconocimiento para canciones nuevas que no tienen por que ser de artistas conocidos, por eso no nos va a aportar nada en el modelo que vamos a entrenar.

- `spotify_id`: no tiene relación porque es simplemente una forma de identificar la canción.

- `name`: creemos que el nombre de la canción no tendrá relevancia porque muchas de las canciones que más escuchamos en nuestro día a día porque se han hecho ``virales'' gracias a las redes sociales no conocemos sus nombres en un principio. 

- `country`: la ubicación geográfica del origen de la canción no tiene una correlación evidente con su popularidad ya que la mayoría de canciones que escuchamos que son populares no sabemos cuál es su origen o no nos interesa para porder disfrutarla.

- `album_name`: muchas de las canciones no más populares no salen ni en un álbum y en general, la gente no se fija en el álbum en sí al que pertenece una canción, simplemente en la canción si les gusta. 

# Apartado B
## Preparación de datos

Para empezar vamos a **eliminar la variable X**, ya que creemos que es una variable nula que se ha colado en la lectura del csv, después vamos a pasar a variables categóricas key y mode ya que actualmente se consideran como numéricas. Y convertiremos las clases `album_release_date` y `snapshot_date` en fechas, en lugar de cadenas para despues poder tratar con ellas.

```{r}
spotify = spotify[,-1]

spotify$mode = factor(spotify$mode)
spotify$key = factor(spotify$key)
spotify$time_signature = factor(spotify$time_signature)

spotify$album_release_date = as.Date(spotify$album_release_date)
spotify$snapshot_date = as.Date(spotify$snapshot_date)

```

A continuación vamos a mirar los **NA's** que presentan nuestras variables.
```{r}
colSums(is.na(spotify))
```

Después de haber mirado un poco por encima el dataset, sabemos que los 21 valores nulos que se muestran con `name` y `artists` son los mismos, al igual que los 22 de `album_release_date` y `album_name`. Además 21 de ellos coinciden como nulos con `name` y `album_name`. 

Aquí podemos ver la cantidad de nulos que coinciden con `name` y `artists`

```{r}
table(is.na(spotify$name), is.na(spotify$artists))
```

Aquí vemos la cantidad de nulos que coinciden con `album_release_date` y `album_name` 

```{r}
table(is.na(spotify$album_release_date), is.na(spotify$album_name))
```

y aquí la cantidad de nulos que coinciden con `name`y `album_release_date`

```{r}
table(is.na(spotify$name), is.na(spotify$album_name))
```
Lo que vamos a realizar es eliminar los 22 valores nulos de estas distintas filas, ya que consideramos que son pocos los datos de los que vamos a prescindir. Es crucial señalar que optaremos por prescindir de los NAs, también porque planeamos trabajar con ellos, ya sea agrupándolos o restando, y la presencia de NAs en la variable `album_release_date` podría afectar negativamente nuestros análisis o cálculos.

```{r}
spotify <- spotify[complete.cases(spotify$album_release_date), ]

colSums(is.na(spotify))
```
Ahora podemos ver que solo quedan 1452 nulos en la columna `country` del dataset. La estrategia que vamos a utilizar es omitir estas filas también, ya que contando actualmente con 105647 datos, creemos que podemos prescindir de estos 1452 nulos, dejándonos así con 104195 valores para nuestro trabajo de investigación.

```{r}
table(is.na(spotify$country))
spotify <- spotify[complete.cases(spotify$country), ]
```
A continuación, vamos a ver si tenemos **outliers** y en caso de verlos, los eliminaremos ya que queremos trabajar con los datos más representativos y cercanos entre sí, dejando así un mejor dataset para la predicción.

```{r}
analysisContinuas= spotify[ , c("popularity", "duration_ms", "danceability","energy", "loudness", "speechiness", "acousticness", "liveness", "instrumentalness", "valence", "tempo")]
mymelt=melt(analysisContinuas, id.vars=1, value.name="FeatureValue", variable.name="Feature")


unique_features <- unique(mymelt$Feature)
num_columns <- 5
num_rows <- ceiling(length(unique_features) / num_columns)
plots_list <- list()

# Tamaño total de la cuadrícula
total_width <- 15
total_height <- 20
single_plot_width <- total_width / num_columns
single_plot_height <- total_height / num_rows

# Itera a través de cada característica
for (i in seq_along(unique_features)) {
  subset_data <- mymelt[mymelt$Feature == unique_features[i], ]
  current_color <- colorspie[(i - 1) %% length(colorspie) + 1]
  
  # Crea el gráfico con ggplot usando los datos filtrados
  plot <- ggplot(data=subset_data, aes(x=Feature, y=FeatureValue, color=Feature, fill=Feature)) +
    geom_boxplot(alpha=0.6) +
    scale_fill_manual(values = current_color) +
    scale_color_manual(values = current_color) +
    ggtitle(paste(unique_features[i])) +
    theme_minimal() + 
    theme(legend.position="none",
          axis.title.x = element_blank(),  # Oculta la etiqueta del eje x
          axis.title.y = element_blank())
  
  # Agrega el gráfico a la lista
  plots_list[[i]] <- plot
  
  summary(mymelt)
}

# Organiza los gráficos en una cuadrícula y visualízalos
grid.arrange(grobs = plots_list, ncol = num_columns, widths = rep(single_plot_width, num_columns), heights = rep(single_plot_height, num_rows))

```

Un dato será *outlier* si se encuentra más allá de 3/2 del rango de intercuartil
```{r}
analysisContinuas= spotify[ , c( "duration_ms", "danceability","energy", "loudness", "speechiness", "acousticness", "liveness", "instrumentalness", "valence", "tempo")]

iqr_values <- apply(analysisContinuas, 2, function(x) {
  q1 <- quantile(x, 0.25)
  q3 <- quantile(x, 0.75)
  iqr <- q3 - q1
  limite_inferior <- q1 - 1.5 * iqr
  limite_superior <- q3 + 1.5 * iqr
  
  outliers_count <- sum(x < limite_inferior | x > limite_superior)
  
  return(list(Q1 = q1, Q3 = q3, IQR = iqr, LimiteInferior = limite_inferior, LimiteSuperior = limite_superior, Outliers = outliers_count))
})

for (i in seq_along(iqr_values)) {
  cat("Variable:", names(iqr_values)[i], "\n")
  cat("  Primer cuartil (Q1):", iqr_values[[i]]$Q1, "\n")
  cat("  Tercer cuartil (Q3):", iqr_values[[i]]$Q3, "\n")
  cat("  IQR:", iqr_values[[i]]$IQR, "\n")
  cat("  Límite inferior:", iqr_values[[i]]$LimiteInferior, "\n")
  cat("  Límite superior:", iqr_values[[i]]$LimiteSuperior, "\n")
  cat("  Número de outliers", iqr_values[[i]]$Outliers, "\n")
  cat("\n\n")
}
```

A pesar de ver una gran cantidad de datos outliers al hacer el cálculo teórico, creemos que no son datos que haya que borrar porque hay muchos juntos como ocurre en la gráfica con `instrumentalness`, que por la representación se ve que no hay ningún punto de datos que destaque por estar muy lejano al resto. 

Por otra parte, apoyándonos en los datos visuales de la gráfica y en los que acabamos de obtener, vamos a eliminar algunos valores de: `duration_ms`, `loudness` y `speechiness`.

```{r}
spotifyEliminacion <- spotify
# Marcamos los datos que eliminaremos de duration_ms
spotifyEliminacion$duration_delete <- if_else(spotifyEliminacion$duration_ms > 570000, TRUE, FALSE)
sum(spotifyEliminacion$duration_ms > 570000)
# Marcamos los datos de loudness
spotifyEliminacion$loudness_delete <- if_else(spotifyEliminacion$loudness > 0 | spotifyEliminacion$loudness< -25, TRUE, FALSE)
sum(spotifyEliminacion$loudness< -25 | spotifyEliminacion$loudness > 0)
# Marcamos los datos de speechiness
spotifyEliminacion$speechiness_delete <- if_else(spotifyEliminacion$speechiness > 0.7, TRUE, FALSE)
sum(spotifyEliminacion$speechiness > 0.7)

# Eliminamos estas filas
spotifyEliminacion <- spotifyEliminacion %>%
  filter(!(duration_delete | loudness_delete | speechiness_delete))
spotify <- spotifyEliminacion[, 1:25]

rm(spotifyEliminacion); rm(subset_data); 
```


**Respóndase a las siguientes preguntas en relación a la preparación de datos.**

\* **¿Qué predictores habría que normalizar? ¿Por qué? ¿Cuál sería la estrategia de normalización en cada caso?**

Los predictores que habría que normalizar son principalmente las características de las canción, para ello se podría utilizar el método **min-max** o la estandarización.
```{r}
songFeatures <-  c('danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms')
```

Normalizar estas características puede ayudar a mejorar convergencia y rendimiento del modelo. Sobretodo si nos encontramos con magnitudes distintas como es por ejemplo duration_ms con respecto a danceability. Así la influencia de estas características en futuros modelos, como el de deep learning será similar y no tendrán tanto peso aquellas con magnitudes tan grandes.

```{r}
spotiNormalizado = spotify
min_max_normalize <- function(x) {
  (x - min(x)) / (max(x) - min(x)) * 2 - 1
}
spotiNormalizado[, songFeatures] <- lapply(spotiNormalizado[, songFeatures], min_max_normalize)
```

Además, creemos que sería interesante escalar debido al uso que le vamos a dar más adelante variables como `daily_movement`, `weekly_movement` y, como `daily_rank` tiene relación con las otras también la vamos a escalar.

```{r}
masDatos <- c('daily_movement', 'weekly_movement', 'daily_rank') 

spotiNormalizado[, masDatos] <- lapply(spotiNormalizado[, masDatos  ], min_max_normalize)
```

También vamos a comprobarlo utilizando el escalado de **z-score** pensando de cara a la próxima parte de la práctica y sabiendo de la existencia de valores negativos, creemos que es mejor hacerlo mediante el z-score. Para ello vamos a utilizar la función `scale` disponible en R.

```{r}
spotiNormalizadoZScore = spotify

songFeaturestoNorm = c('danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms', 'daily_movement', 'weekly_movement', 'daily_rank')

spotiNormalizadoZScore[, songFeaturestoNorm] <- scale(spotiNormalizadoZScore[, songFeaturestoNorm],center = TRUE, scale = TRUE)
```


\* **¿Podría ser interesante transformar algún atributo o grupos de atributos en uno nuevo? ¿Por qué?**

Sí, lo que vamos a hacer es combinar las variables `album_release_date` y `snapshot_date` dando lugar a `diferencia_dias` y añadir los atributos `between_week_prev_popularity` y `day_prev_popularity` para tener guardados los datos de la popularidad previa, aprovechando así la secuencialidad.

Con `diferencia_dias` vamos a tener en cuenta los días que hay desde que se publica una canción hasta que esta obtiene una nueva posición en el ranking, la forma en la que podemos hacer esto es restar `snapshot_date` con `album_release_date` y tener así un registro del número de **días que una canción tardó en hacerse popular**, como la fecha de salida de una canción puede ser anterior a la salida de un álbum pondremos como 0 aquellas canciones que tienen como diferencia de fechas un valor negativo.

```{r}
diferencia_dias = as.numeric(spotify$snapshot_date - spotify$album_release_date)

spotify$diferencia_dias = diferencia_dias
spotify$diferencia_dias = ifelse(spotify$diferencia_dias < 0, 0, spotify$diferencia_dias)
```

Ahora vamos a calcular la **popularidad previa** de canciones en un conjunto de datos.

```{r secuencial}
spotify <- spotify %>%
  group_by(spotify_id,country) %>%
  arrange(snapshot_date) %>%
  mutate(between_week_prev_popularity = if_else(row_number() == 1, popularity, if_else((snapshot_date - lag(snapshot_date)>1 & (snapshot_date - lag(snapshot_date)<=7)) , lag(popularity), 0)))

spotify <- spotify %>%
  group_by(spotify_id,country) %>%
  arrange(snapshot_date) %>%
  mutate(day_prev_popularity = if_else(row_number() == 1, 0, if_else(snapshot_date - lag(snapshot_date) == 1, lag(popularity), 0)))


```

Primero organizamos la información por identificador y país, asegurándose de que los registros estén ordenados; luego, para cada canción se determina la popularidad que tenía en fechas anteriores, considerando únicamente aquellas con una diferencia de 1 a 7 días respecto a la fecha actual. El resultado es una nueva columna llamada `between_week_prev_popularity`, que refleja la popularidad previa de cada canción en el tiempo. Tambien se hace el mismo proceso para solamente el día anterior. Para la primera aparición por lista de cada canción se asigna su propia popularidad porque de no ser así nos saldria un valor NA.


También podríamos considerar la creación de una nueva varibale que represente la rapidez con la que una canción se vuelve popular. Por ejemplo, podríamos crear una variable `velocidad_popularidad` que sea el inverso de la diferencia de días, de manera que canciones que se vuelven populares rápidamente tengan valores más altos en esta nueva variable.

```{r}
epsilon <- 1e-10
# Calcular velocidad_popularidad con epsilon para evitar divisiones entre 0
spotify$velocidad_popularidad <-  1 / (spotify$diferencia_dias + epsilon)


plot(spotify$popularity, spotify$velocidad_popularidad, 
     xlab = "Popularidad", 
     ylab = "Velocidad de Popularidad",
     main = "Relación entre Velocidad y Popularidad",
     ylim = c(0, 1),  # Establecer límites en el eje y
     col = colorspie,
     pch=19)

```
En la gráfica se ve cómo claramente las canciones que más rápido se hacen conocidas son las más populares.

\* **¿Cómo podría aprovecharse el carácter secuencial de los datos?**

El carácter secuancial de los datos podría aprovechase al considerar la creación de variables que capturen tendencias a lo largo del tiempo. Por ejemplo, un poco más arriba hemos creado una variable `prev_popularity` gracias a que las fechas son secuenciales y podemos acceder a la popularidad a lo largo de los días. De esta forma podemos analizar cómo cambia la popularidad de una canción a lo largo de la semana anterior, proporcionando información valiosa para entender patrones y tendencias en el comportamiento de reproducción de las canciones en la plataforma.


Antes de empezar el apartado C, vamos a eliminar finalmente los atributos que creemos que no nos van a ser de utilidad para realizar nuestros modelos. Para ello primero vamos a hacer un repaso con los pasos seguidos anteriormente:
- Se eliminaron valores nulos, para poder hacer agrupaciones sin preocuparnos de los NA'S
- Se eliminaron "outliers", aunque los eliminados fueron nuestra decisión, ya que vimos que había demasiados datos considerados outliers en distintos valores fuera del IQR que nos parecian representativos y necesarios. 





Hasta ahora no hemos hecho nada más que añadir columnas, pero sabemos de antemano que no todas ellas nos van a ser de utilidad para entrenar nuestro modelo. Sin embargo, no las hemos eliminado anteriormente para hacer la limpieza de datos pero ahora procedemos a quitar las columnas `spotify_id`, `name`, `artists`, `album_name`, `album_release_date`   

```{r}
eliminar = c("spotify_id", "name", "artists", "album_name", "album_release_date")
```


Anteriormente hemos agrupado los argumentos `album_release_date` y `snapshot_date` en la variable que recoge la diferencia de días. Por lo que ahora también consideramos oportuno quitar la variable `snapshot_date` ya que ya tenemos en el dataset el número de días que la canción ha tardado en tener la respectiva `popularity`.


```{r}
eliminar = c("spotify_id", "name", "artists", "album_name", "album_release_date","snapshot_date")
```


Cómo explicábamos antes, la variable country no nos va a ser de utilidad a determinar si una canción va a ser popular o no. Por otro lado somos conscientes de que el país si que afectaría a que tipo de canciones se vuelven o no populares, pero al igual que otros atributos como el artista (que ya hemos eliminado previamente). Como el objetivo es dada una canción saber si esta se va a hacer famosa o no, no vamos a tener en cuenta el país o continente.

```{r}
eliminar = c("spotify_id", "name", "artists", "album_name", "album_release_date","snapshot_date", "country")
```

Hasta ahora hemos borrado los parámetros que nos parecían más evidentes pero pensando en la resolución del problema, nosotras también llegamos a la conclusión de que los parámetros `daily_movement`, `daily_rank` y `weekly_movement` a pesar de que nos aportarán algo y sabemos que están relacionados con la popularidad, no nos ayudarán a determinar si una canción cualquiera se va a hacer famosa (al igual que con el parámetro `artists`). 

Hemos llegado a esta conclusión debido a que el pensamiento que tenemos nosotras al hacer esta práctica es que vamos a crear un proyecto para determinar si una cancion será popular o no, y estos tres parámetros nos dan información sobre canciones en la lista pero nada más. Para confirmar nuestras sospechas trabajaremos con un dataset conjunto que sí que tenga estos tres parámetros y después comprobaremos la precisión.

Nuestra hipótesis sobre estos parámetros es que sí tienen cierta importancia y que puestos más altos en el daily rank supondrán una mayor popularidad pero creemos que no nos servirá de mucho a la hora de determinar la posible popularidad de una canción futura.

```{r}
spotify = spotify[, !names(spotify) %in% eliminar]
spotiNormalizado = spotiNormalizado[, !names(spotiNormalizado) %in% eliminar]
spotiNormalizadoZScore = spotiNormalizadoZScore[, !names(spotiNormalizadoZScore) %in% eliminar]

nuevas_columnas = c('diferencia_dias', 'between_week_prev_popularity','day_prev_popularity', 'velocidad_popularidad')
spotiNormalizado[nuevas_columnas] <- spotify[, nuevas_columnas]
spotiNormalizadoZScore[nuevas_columnas] <- spotify[, nuevas_columnas]

spotifyNoMovements = spotify

eliminar = c("daily_rank", "daily_movement", "weekly_movement")
spotifyNoMovements = spotifyNoMovements[, !names(spotify) %in% eliminar]

spotifyNormNoMovements = spotiNormalizado
spotifyNormNoMovements = spotiNormalizado[,!names(spotiNormalizado) %in% eliminar ]

spotifyNormNoMovementsZScore = spotiNormalizadoZScore
spotifyNormNoMovementsZScore = spotiNormalizadoZScore[,!names(spotiNormalizado) %in% eliminar ]
```


```{r}
#SAVE POINT PARA CUANDO LA CAGO. BORRAR  Y LO DE ABAJO 
savePoint = spotify
```

creemos que saber de antemano la popularidad exacta de una canción va a ser muy complicado. Creemos que es mejor idea hacer una factor de la popularidad donde se adivine si esta va a ser muy popular (su valor de popularidad es de entre 90-100), si va a ser popular (su valor de popularidad es de entre 60-90) o si no va a ser muy popular (su valor de popularidad es de entre 30-60) o si apenas lo va a ser (0-30). Porque pensamos que el objetivo, es saber si la canción va a ser buena o no y preferimos abordar un problema de clasificación a un problema de regresión, ya que en caso de llevar a la realidad, creemos que interesaría más darle a conocer este dato a un número de popularidad más exacta, por lo que, si la otra idea no es abordable llevaremos a cabo una que trabaje con la variable `popularity`, factorizada, de la siguiente manera:

```{r, eval=FALSE}
intervalos <- c(0, 30, 60, 90, 101)
spotify2 = spotify
spotify2$popularity <- cut(spotify2$popularity, breaks = intervalos, labels = c("nada popular", "poco popular", "algo popular", "hit"), include.lowest = TRUE, right = FALSE)
spotify2$popularity = as.factor(spotify2$popularity)
```


# Apartado C
## Random Forest
### Funcionamiento

**Explicar brevemente el tipo de modelo que genera el algoritmo, y cuál es la estrategia de dicho algoritmos para construir el modelo.**

El algoritmo **Random Forest** se utiliza para tareas de clasificaión y de regresión. Su estructura consiste en un conjunto de árboles de decisión, de ahí que se denomine ``bosque''. La estrategia principal del Random Forest se basa en la combinación de múltiples árboles de decisión mediante el siguiente proceso:

1. **Muestreo aleatorio de datos:** Se toma el conjunto de entrenamiento y se crean múltiples subconjuntos mediante muestreo aleatorio con reemplazo.

2. **Construcción de árbolres de decisión:** Cada subconjunto se utiliza para entrenar un árbol de decisión mediante un proceso recursivo de partición basado en la mejor característica.

3. **Valoración:** Se realiza la predicción para cada árbol y, en clasificación, se realiza una votación para determinar la clase final y en regresión, se promedian las predicciones.

La idea clave es combinar la información de múltiples árboles para reducir sobreajuste y mejorar la generalización.


### Requisitos
**Indicar si el algoritmo en cuestión tiene algún requisito en cuanto a si se han de preprocesar los datos (e.g. escalado, imputación de valores nulos, etc.) y cómo. Explicar cómo se ha tenido en cuenta estos requisitos a la hora de generar los datos de training específicos para este algoritmo.**

Random Forest no tiene requisitos estrictos en cuanto al preprocesamiento de datos. Sin embargo, cambios que hemos hecho en los datos ayudarán al rendimiento como puede ser la eliminación de datos no relevantes y el manejo de los valores nulos.

Tal y como vimos en clase vamos a utilizar la función ranger, debido a su rapidez en la implementación de bosques aleatorios.

### Descripción de hiperparámetros
**Identificar y explicar cada uno de sus parámetros de configuración.**

Los hiperparámetros de configuración son los siguientes:

```{r}
rangerInfo = getModelInfo("ranger")
rangerInfo = rangerInfo$ranger
rangerInfo$parameters
```
Aquí podemos ver los parámetros `mtry`, `splitrule`, `min.node.size`.

Donde `mtry` es el número de variables a considerar en cada división del árbol, hemos leido que se suele probar con la raiz cuadrada del número de variables, por lo que nosotras vamos a probar con valores que van de 4 a 6.

Por otro lado `splitrule` es la regla que se utiliza para dividir cada nodo. Al tratarse de un problema de regresión, estamos probando con variance y con extratree.
Y por ultimo `min.node.size` que es el numero de ejemplos que debe haber como minimo en cada nodo. En nuestro caso estaremos probando con 50,100 y 150.

### Grid Hiperparámetros
**Detallar una estrategia para la generación del grid de valores para hiperparámetros a usar.**

A continuacion se puede ver como hemos generado un grid, implementando lo que comentamos en el apartado anterior y más adelante como hemos probado con distintos números de árboles; para los valores 200,300,400 y 500. De esta forma veremos si el número de árboles es significativo.

```{r}
mygrid <- expand.grid(
  mtry = c(4, 5, 6),
  splitrule = c("variance", "extratrees"),
  min.node.size = c(50, 100, 150))

fitControl <- trainControl(## 10-fold CV
  method = "cv",
  number = 10)
```


En el primer chunk donde entrenamos un modelo, se puede ver además comentado, que antes de ejecutar el codigo con todos los datos, se hizo también para un porcentaje de datos de la muestra. En nuestro caso fue del 30%. Al encontrar resultados significativos con el grid pensado, hemos decidido seguir adelante con él y más abajo se contrastaran los resultados y se probara que al menos una de las configuraciones nos produce unos resultados aceptables. 

```{r}
set.seed(123)
inTraining <- createDataPartition(spotify$popularity, times=1,p = .80, list = FALSE)
training <- spotify[ inTraining,]
testing  <- spotify[-inTraining,]
```

```{r}
spTraining = training
spTesting = testing
```


```{r rf200trees, eval=FALSE}
set.seed(44)
#porcentaje_datos <- 0.30
#tamanio_muestra <- round(nrow(spotify) * porcentaje_datos)
#spotify = spotify[sample(nrow(spotify), tamanio_muestra), ]


rf_200t <- train(popularity ~ ., data = training, 
                 method = "ranger", 
                 trControl = fitControl,
                 num.trees = 200,
                 tuneGrid=mygrid)

saveRDS(object= rf_200t, file = "numTrees200.rds")
```

```{r rf300trees, eval= FALSE}
rf_300t <- train(popularity ~ ., data = training, 
                 method = "ranger", 
                 trControl = fitControl,
                 num.trees = 300,
                 tuneGrid=mygrid)

saveRDS(object= rf_300t, file = "numTrees300.rds")
```

```{r rf400trees, eval = FALSE}
rf_400t <- train(popularity ~ ., data = training, 
                 method = "ranger", 
                 trControl = fitControl,
                 num.trees = 400,
                 tuneGrid=mygrid)

saveRDS(object= rf_400t, file = "numTrees400.rds")
```

```{r rf500trees, eval = FALSE}
rf_500t <- train(popularity ~ ., data = training, 
                 method = "ranger", 
                 trControl = fitControl,
                 num.trees = 500,
                 tuneGrid=mygrid)

saveRDS(object= rf_500t, file = "numTrees500.rds")
```

```{r}
rf_200t = readRDS("rf/numTrees200.rds")
rf_300t = readRDS("rf/numTrees300.rds")
rf_400t = readRDS("rf/numTrees400.rds")
rf_500t = readRDS("rf/numTrees500.rds")

resultados <- data.frame(
  "NumArboles" = c("200", "300", "400", "500"),
  "Splitrule" = c(rf_200t$bestTune$splitrule, rf_300t$bestTune$splitrule, rf_400t$bestTune$splitrule, rf_500t$bestTune$splitrule),
  "Min node size" = c(rf_200t$bestTune$min.node.size, rf_300t$bestTune$min.node.size, rf_400t$bestTune$min.node.size, rf_500t$bestTune$min.node.size),
  "Mtry" = c(rf_200t$bestTune$mtry, rf_300t$bestTune$mtry, rf_400t$bestTune$mtry, rf_500t$bestTune$mtry)
)
resultados
```

Podemos para todas las opciones de número de árboles que hemos contemplado obtenemos la misma configuración.

```{r}
# Función para calcular MAE y RMSE

calcular_metricas <- function(modelo, test) {
  predicciones <- predict(modelo, newdata = test)
  mae <- mae(predicciones, testing$popularity)
  rmse <- rmse(predicciones, testing$popularity)
  return(c(MAE = mae, RMSE = rmse))
}

# Comparación MAE y RMSE
comparacion <- data.frame(
  rf_200t = calcular_metricas(rf_200t, spTesting),
  rf_300t = calcular_metricas(rf_300t, spTesting),
  rf_400t = calcular_metricas(rf_400t, spTesting),
  rf_500t = calcular_metricas(rf_500t, spTesting)
)
comparacion
```


Para este dataset, en lugar de ejecutarlo con distintas posibilidades, como en todos los anteriores nos ha dado la misma combinación, para este solo vamos a utilizar esa en concreto con un número de 300 arboles. Recordamos que este dataset, no tiene los atributos de `daily_rank`, `daily_movement`, `weekly_movement`.

```{r}
training <- spotifyNoMovements[ inTraining,]
testing <- spotifyNoMovements[ -inTraining,]
```

```{r, eval=FALSE}
fitControl <- trainControl(## 10-fold CV
  method = "cv",
  number = 10)

mygrid <- expand.grid(
  mtry = 6,
  splitrule = c("variance"),
  min.node.size = 50)
mygrid

fitControl <- trainControl(## 10-fold CV
  method = "cv",
  number = 10)

spotifyrf <-train (popularity ~ ., data = training,
                   method = "ranger",
                   trControl = fitControl,
                   num.trees = 300,
                   tuneGrid=mygrid)

saveRDS(object= spotifyrf, file = "spotifyNoMovements.rds")
```

```{r }
rfNoMov = readRDS("rf/spotifyNoMovements.rds")
```

Estos son el MAE y el RMSE de este modelo
```{r}
rfNoMov_result = calcular_metricas(rfNoMov, testing)
rfNoMov_result
```

Ahora vamos a verlo junto con el mejor modelo anterior.

```{r}
comparacion = data.frame(
  rf_300t = calcular_metricas(rf_300t, spTesting),
  rfNoMov = calcular_metricas(rfNoMov, testing)
)
comparacion
```

Como pensábamos, el modelo sin los valores de `daily_movement`, `daily_rank` y `weekly_movement` ha tenido unos resultados más bajos para el RMSE y el MAE, lo que nos inidica que tiene un mejor rendimiento.

Tras ver los resultados del apartado anterior, podemos describir los hiperpárametros escogidos del modelo:

- *Splitrule:* Este parámetro determina la regla utilizada para dividir los nodos en los árboles de decisión. En este caso, se ha elegido la opción `variance.` La regla de división basada en la varianza busca maximizar la homogeneidad de las variables en los nodos del árbol.

- *Min.node.size:*  Este hiperparámetro establece el número mínimo de observaciones requeridas en un nodo terminal del árbol. En el contexto de Random Forest, un valor de 50 para `min.node.size` significa que un nodo terminal debe contener al menos 50 observaciones para considerarse válido. Este valor puede ayudar a controlar la complejidad del árbol y evitar sobreajustes.

- *Mtry:* Este parámetro controla el número de variables aleatorias consideradas en cada división de nodo. Un valor de 6 para `mtry` implica que, en cada división, el algoritmo seleccionará aleatoriamente 6 variables predictoras del conjunto total de variables.

La elección de estos valores específicos para los hiperparámetros se basa en los resultados obtenidos durante el análisis en el apartado anterior. Observamos que esta combinación en particular tuvo un el mejor rendimiento según el RMSE y MAE.

### Resultados
**Describir los resultados del algoritmo.**

Con las conclusiones anteriores, el que pensamos que es el mejor modelo es `rfNoMov`. Como hemos comentado, la configuración de este modelo es:

- Splitrule: variance
- Min.node.size: 50
- Mtry: 6

El modelo se entrenó con *300 árboles*, y los resultados fueron los siguientes:

- MAE (Error Absoluto Medio): 0.5488207
- RMSE (Error Cuadrático Medio): 1.989452

Estos resultados indican que el modelo tiene capacidad para hacer predicciones precisas sobre la popularidad de las canciones en función de las variables disponibles. En comparación con los modelos anteriores que incluían las variables excluidas, este modelo reducido logró un rendimiento aún mejor, lo que respalda la decisión de prescindir de esas características específicas en este contexto.



## Deep learning
### Funcionamiento
**Explicar brevemente el tipo de modelo que genera el algoritmo, y cuál es la estrategia de dicho algoritmos para construir el modelo.**

El **Deep Learning** se utiliza en redes neuronales profundas para modelar y resolver problemas complejos. Una red neuronal profunda consta de múltiples capas, incluyendo capas de entrada, capas ocultas y capas de salida por las que van pasando los datos. La estrategia es la siguiente:

1. **Propagación hacia adelante (Forward Propagation):** Los datos se introducen en la red y se propagan hacia adelante a través de las capas, generando una predicción.

2. **Cálculo de pérdida:** Se compara la predicción con las etiquetas reales para calcular la pérdida.

3. **Retropropagación (Backward Propagation):** Se propaga hacia atrás la pérdida a través de la red, ajustando los pesos de las conexiones para minimizar la pérdida.

4. **Optimización:** Se utiliza un algoritmo de optimización *(como Gradiente Descendente)* para actualizar los pesos y minimizar la pérdida.

Este proceso se repite en varias épocas (iteraciones a través del conjunto de datos) durante el entrenamiento.

Nosotras, en cambio, al utilizar R vamos a tener un funcionamiento diferente.

En R, el algoritmo **mlpKeras** se basa en la construcción de redes neuronales artificiales utilizando la interfaz de Keras. Aquí está la descripción del funcionamiento, junto con las estrategias específicas relacionadas con **Dropout** y **Decay**:

1. **Construcción del modelo:** Se define la arquitectura del modelo utilizando la función `keras_model_sequential()`. Se añaden capas mediante el método `%>%`, especificando el número de unidades y la función de activación en cada capa.

2. **Capa Dropout (Dropout Layer):** Si se utiliza Dropout en el modelo (`mlpKerasDropout`), se añade una capa de Dropout después de las capas ocultas. La capa Dropout apaga aleatoriamente un porcentaje de unidades durante el entrenamiento, lo que ayuda a prevenir el sobreajuste al introducir variabilidad en la red.

3. **Capa Decay (Decay):** Si se utiliza Decay en el modelo (`mlpKerasDecay`), se incorpora regularización L2 mediante el parámetro `lambda`. La regularización L2 penaliza los pesos grandes, evitando así el sobreajuste y mejorando la generalización del modelo. Nosotras utilizaremos dropout o decay por separado para comparar el rendimiento.


### Requisitos
**Indicar si el algoritmo en cuestión tiene algún requisito en cuanto a si se han de preprocesar los datos (e.g. escalado, imputación de valores nulos, etc.) y cómo. Explicar cómo se ha tenido en cuenta estos requisitos a la hora de generar los datos de training específicos para este algoritmo.**

Los requisitos para el Deep Learning incluyen:

- Normalización de variables: para que tengan una media cercana a cero y una desviación estándar similar.

- Manejo de valores nulos: los datos deben estar limpios de valores nulos antes del entrenamiento.

- Codificación de variables categóricas: las variables categóricas deben codificarse adecuadamente para ser utilizadas en la red neuronal.

Aunque hemos tenido en cuenta estos requisitios en la preparación de datos, hemos modificado un podo el conjunto de datos por lo que vamos a terminar de arreglarlo para poder utilizarlo.

```{r}
restantesNormal = c('velocidad_popularidad', 'diferencia_dias', 'between_week_prev_popularity', 'day_prev_popularity')
spotiNormalizado[, restantesNormal] <- lapply(spotiNormalizado[, restantesNormal], min_max_normalize)
spotifyNormNoMovements[, restantesNormal] <- lapply(spotifyNormNoMovements[, restantesNormal], min_max_normalize)
spotiNormalizadoZScore[, restantesNormal] = scale(spotiNormalizadoZScore[, restantesNormal],center = TRUE, scale = TRUE)
spotifyNormNoMovementsZScore[, restantesNormal] = scale(spotifyNormNoMovementsZScore[, restantesNormal],center = TRUE, scale = TRUE)
```
A continuacion tenemos que tratar tambien con las variables categóricas `key`, `mode` , `time_signature`, `is_exlicit`. Para ello vamos a utilizar la tecnica one-Hot-Encoding. Que nos permitira representar numéricamente variables categóricas y manejar variables categóricas con múltiples categorías, estas variables no las tendremos que normalizar, ya que hemos generado una columna nueva para cada posible valor de las columnas `key`, `mode`y `time_signature`, poniendo como valor 1 si coincide el valor original con el nombre de la columna.
`
```{r, include = FALSE}
spSpotiNormalizado = spotiNormalizado
spSpotifyNormNoMovements = spotifyNormNoMovements
spSpotiNormalizadoZScore = spotiNormalizadoZScore
spSpotifyNormNoMovementsZScore = spotifyNormNoMovementsZScore
```

```{r}

key_values <- c(0,1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)

for (key_value in key_values) {
  column_name <- paste0("key_", key_value)
  spotiNormalizado[column_name] <- as.integer(spotiNormalizado$key == key_value)
  spotifyNormNoMovements[column_name] <- as.integer(spotifyNormNoMovements$key == key_value)
  spotiNormalizadoZScore[column_name] <- as.integer(spotiNormalizadoZScore$key == key_value)
  spotifyNormNoMovementsZScore[column_name] <- as.integer(spotifyNormNoMovementsZScore$key == key_value)
}

time_signatures <- c(1,3,4,5)

for (time_signature_v in time_signatures) {
  column_name <- paste0("time_signature_", time_signature_v)
  spotiNormalizado[column_name] <- as.integer(spotiNormalizado$time_signature == time_signature_v)
  spotifyNormNoMovements[column_name] <- as.integer(spotifyNormNoMovements$time_signature == time_signature_v)
  spotiNormalizadoZScore[column_name] <- as.integer(spotiNormalizadoZScore$time_signature == time_signature_v)
  spotifyNormNoMovementsZScore[column_name] <- as.integer(spotifyNormNoMovementsZScore$time_signature == time_signature_v)
}

spotiNormalizado$is_explicit = ifelse(spotiNormalizado$is_explicit, 1, 0)
spotifyNormNoMovements$is_explicit = ifelse(spotifyNormNoMovements$is_explicit, 1, 0)
spotiNormalizadoZScore$is_explicit = ifelse(spotiNormalizadoZScore$is_explicit, 1, 0)
spotifyNormNoMovementsZScore$is_explicit = ifelse(spotifyNormNoMovementsZScore$is_explicit, 1, 0)


modes <- c(0,1)

for (mode_value in modes) {
  column_name <- paste0("mode_", mode_value)
  spotiNormalizado[column_name] <- as.integer(spotiNormalizado$mode == mode_value)
  spotifyNormNoMovements[column_name] <- as.integer(spotifyNormNoMovements$mode == mode_value)
  spotiNormalizadoZScore[column_name] <- as.integer(spotiNormalizadoZScore$mode == mode_value)
  spotifyNormNoMovementsZScore[column_name] <- as.integer(spotifyNormNoMovementsZScore$mode == mode_value)
  
}

eliminar = c('mode', 'key', 'is_explicit', 'time_signature')
spotiNormalizado = spotiNormalizado[, !names(spotiNormalizado) %in% eliminar]
spotifyNormNoMovements = spotifyNormNoMovements[, !names(spotifyNormNoMovements) %in% eliminar]
spotiNormalizadoZScore = spotiNormalizadoZScore[, !names(spotiNormalizadoZScore) %in% eliminar]
spotifyNormNoMovementsZScore = spotifyNormNoMovementsZScore[, !names(spotifyNormNoMovementsZScore) %in% eliminar]

```

```{r, include=FALSE}
spSpotiNormalizado2 = spotiNormalizado
spSpotifyNormNoMovements2 = spotifyNormNoMovements
spSpotiNormalizadoZScore2 = spotiNormalizadoZScore
spSpotifyNormNoMovementsZScore2 = spotifyNormNoMovementsZScore
```


### Descripción de hiperparámetros
**Identificar y explicar cada uno de sus parámetros de configuración.**

Se consideran para la resolución del modelo `mlpKerasDropout` y `mlpKerasDecay`, creemos que los modelos de los costes no son los más acertados, debido a que no nos interesa tener como hiperparámetros costes para el modelo que estamos resolviendo.

```{r}
mlpDropoutInfo = getModelInfo("mlpKerasDropout")

mlpDecayInfo = getModelInfo("mlpKerasDecay")
mlpHiper = data.frame(mlpDropoutInfo$mlpKerasDropout$parameters, mlpDecayInfo$mlpKerasDecay$parameters)
mlpHiper
```

Podemos ver como ambas funciones tienen en común los parámetros `size`, `batch_size`, `lr`, `rho`, `decay`, `activation`
- size: Número de neuronas en una capa.
- batch_size: Cantidad de datos utilizados en cada actualización del modelo durante el entrenamiento.
- lr (tasa de aprendizaje): Controla el tamaño de los pasos durante la optimización.
- rho: Parámetro asociado con métodos de optimización específicos (como en RMSprop).
- decay: Decaimiento de la tasa de aprendizaje a lo largo del tiempo.
- activation: Función de activación aplicada a las salidas de las unidades en una capa.

Por otro lado tenemos `dropout` que pertenece a mlpKerasDropout y `lambda` que pertenece a mlpKerasDecay

-`dropout` La tasa de Dropout es un hiperparámetro que controla la proporción de unidades (neuronas) en una capa que se "apagan" o se excluyen aleatoriamente durante cada paso de entrenamiento. Esto significa que, durante el entrenamiento, un porcentaje de las conexiones en la red se desactivan temporalmente, lo que ayuda a prevenir el sobreajuste y mejora la generalización del modelo.

- `lambda`  El parámetro lambda se asocia con la regularización L2. La regularización L2 agrega un término a la función de pérdida del modelo que penaliza los pesos de las conexiones, evitando que tomen valores extremadamente grandes. Esto ayuda a prevenir el sobreajuste y a mejorar la robustez del modelo.

La prinicpal diferencia radica en:

Dropout está relacionado con la exclusión aleatoria de unidades durante el entrenamiento para evitar el sobreajuste. Es una técnica de regularización específica para las capas ocultas de la red.

Lambda en este contexto se asocia con la regularización L2, que penaliza los pesos de las conexiones. Es una técnica de regularización que afecta directamente a los parámetros del modelo.

Nosotras vamos a realizar experimentos para ambos para ver si resulta significativo utilizar Dropout o Decay

### Grid Hiperparámetros
**Detallar una estrategia para la generación del grid de valores para hiperparámetros a usar.**

#### Dropout

Después de haber visto los parámetros en el apartado anterior vamos a generar una estrategia para el grid.

```{r}
paramgrid2 <- expand.grid(
  size = c(40,50), 
  dropout =c(0.1,0.3),
  batch_size = c(15,20),
  lr = c(0.01,0.02),
  rho = 0.91,
  decay = c(1e-6, 1e-5),
  activation = 'tanh')
```
La mayoría de ellas se han hecho a través de prueba y error. Hay otras de las que no eramos muy conscientes de su existencia, por ejemplo, investigamos que `rho` era un buen valor entre 0.9-0.96. Nosotras hemos usado 0.91 porque era el valor que mejor nos funcionaba con el 30% de los datos. 

Por otro lado hemos escogido `tanh` como la función de activación debido a la existencia de valores negativos y positivos en nuestros datos.

`dropout` va a tomar los valores 0.1 y 0.3 debido a que queremos conocer si repercute la regularidad que le metamos a nuestros datos y hemos leido que son valores normales.

-`lr`, sabemos que lr más bajos puedes ser más lentos y aprender (o incluso sobreaprender más) y que lr más altos pueden lidiar a no aprender del todo bien. Creemos que `0.01`y `0.02` son dos buenas opciones. 

-`decay`: Un parámetro de decaimiento que controla la disminución de la tasa de aprendizaje a lo largo del tiempo. Hemos considerando dos opciones, 1e-6 y 1e-5. 

-Para `size` y `batch size` tambien han sido escogidos a traves de pruebas y errores individuales. Leimos que era buena opción para `size` utilizar el doble de variables, nosotras lo hemos hecho de forma aproximada, ya que 40 y 50 nos devolvian mejores resultados que 60. 
```{r}
set.seed(124)

inTraining <- createDataPartition(spotiNormalizado$popularity, times=1,p = .80, list = FALSE)
training <- spotiNormalizado[ inTraining,]
testing  <- spotiNormalizado[-inTraining,]

training2 <- spotifyNormNoMovements[ inTraining,]
testing2 <- spotifyNormNoMovements [-inTraining,]

# Definir el control del modelo
fitControl <- trainControl(
  method = "cv",  # Cross-validation
  number = 10,     # Número de folds
  
) 

sptesting1 = testing
sptesting2 = testing2
```


Este entrenamiento se ha hecho con 32 combinaciones distintas para el grid y ha conllevado una duración superior a 12 horas, por lo que para los siguientes modelos entrenados con `mlpKerasDropout`se va a mirar cuales han sido las mejores 5 combinaciones de los grids y se van a entrenar de esa manera.

Modelo que contiene `daily_rank`, `daily_movement`, `weekly_movement`. 
```{r, eval=FALSE}

modelo <- train(
  popularity ~ ., 
  data = training,
  method = "mlpKerasDropout", 
  trControl = fitControl,
  tuneGrid = paramgrid2,
  epochs = 25
)

saveRDS(object = modelo, file = "mlpNorm1Dropout.rds")
```
Ahora vamos a ver el modelo obtenido.
```{r}
mlpNorm1Dropout = readRDS("mlpDropout/Regresion/mlpNorm1Dropout.rds")
mlpNorm1Dropout
```
Vemos que las 5 mejores combinaciones tienen el mismo `size`, que es 40, el mismo `dropout`, que es 0.1, y para el `batch_size` hay 4 combinaciones con 15, y 1 con 20. El `lr` oscila tambien entre 0.01 y 0.02, y el decay entre 1e-5 y 1e-6.
Debido al gran tiempo de ejecución que nos ha consumido esta red neuronal vamos a resumir el grid como hemos dicho anteriormente para el entrenamiento posterior de modelos que utlicen Dropout. La creación del nuevo grid es la siguiente:

```{r}
gridDropOut <- expand.grid(
  size = 40, 
  dropout =0.1,
  batch_size = c(15,20),
  lr = c(0.01,0.02),
  rho = 0.91,
  decay = c(1e-6, 1e-5),
  activation = 'tanh'
)
```
Este grid solo tiene 8 combinaciones distintas.


Modelo que no contiene `daily_rank`, `daily_movement`, `weekly_movement`. 
```{r, eval=FALSE}
modelo <- train(
  popularity ~ ., 
  data = training2,
  method = "mlpKerasDropout",  
  trControl = fitControl,
  tuneGrid = gridDropOut,
  epochs = 25
)

saveRDS(object = modelo, file = "mlpNormNoMovDropout.rds")
```

```{r}
mlpNormNoMovDropout_model = readRDS("mlpDropout/Regresion/mlpNormNoMovDropout.rds")
```


```{r}
calcular_metricas <- function(modelo, test) {
  predicciones <- predict(modelo, newdata = test)
  mae <- mae(predicciones, testing$popularity)
  rmse <- rmse(predicciones, testing$popularity)
  return(c(MAE = mae, RMSE = rmse))
}

mlpNorm1Dropout = calcular_metricas(mlpNorm1Dropout, testing)
mlpNormNoMovDropout_model = calcular_metricas(mlpNormNoMovDropout_model, testing2)

```


#### Decay
A continuación tambien se van a entrenar redes neuronlaes utilizando Decay, para ello vamos a utilizar el mismo grid usado para el primer entrenamiento de la red neuronal (el de 32 combinaciones), pero modificando el parámetro de `dropout` por el de `lambda`, va a tomar los valores 0.001 y 0.003, los demás valores van a ser los mismos vistos anteriormente.

```{r}
paramgrid2 <- expand.grid(
  size = c(40,50), 
  lambda =c(0.001,0.003),
  batch_size = c(15,20),
  lr = c(0.01,0.02),
  rho =0.91,
  decay = c(1e-6, 1e-5),
  activation = 'tanh'
)
```

Entrenamos el modelo con esta combinacion
```{r, eval=FALSE}
modelo <- train(
  popularity ~ ., 
  data = training,
  method = "mlpKerasDecay",  
  trControl = fitControl,
  tuneGrid = paramgrid2,
  epochs = 25
)

saveRDS(object = modelo, file = "mlpNorm1Decay.rds")
```

```{r}
mlpNorm1Decay_model = readRDS("mlpDecay/Regresion/mlpNorm1Decay.rds")
predicciones1 = mlpNorm1Decay_model
mlpNorm1DecayModel = predicciones1 
```

Establecemos el nuevo grid para que las proximas ejecuciones no tarden tanto a uno de 8 combinaciones. Al igual que hicimos anteriormente.
```{r}
gridDecay <- expand.grid(
  size = 40, 
  lambda =0.001,
  batch_size = c(15,20),
  lr = c(0.01,0.02),
  rho = 0.91,
  decay = c(1e-6, 1e-5),
  activation = 'tanh'
)
```

```{r, eval=FALSE}
modelo <- train(
  popularity ~ ., 
  data = training2,
  method = "mlpKerasDecay",  
  trControl = fitControl,
  tuneGrid = gridDecay,
  epochs = 25
)

saveRDS(object = modelo, file = "mlpNormNoMovDecay.rds")
```

```{r}
mlpNormNoMovDecay_model = readRDS("mlpDecay/Regresion/mlpNormNoMovDecay.rds")
```

```{r}
mlpNorm1Decay_model = calcular_metricas(mlpNorm1Decay_model, testing)
mlpNormNoMovDecay_model = calcular_metricas(mlpNormNoMovDecay_model, testing2)
```

### Normalizado z-score
Utilizando el escalado definido anteriormente vamos a repetir los mismos pasos, utilizando los grids de 8 combinaciones.

Aquí volvemos a hacer una particion del conjunto.
```{r}
set.seed(125)
spotiNormalizado = spotiNormalizadoZScore
spotifyNormNoMovements = spotifyNormNoMovementsZScore

inTraining <- createDataPartition(spotiNormalizado$popularity, times=1,p = .80, list = FALSE)
training <- spotiNormalizado[ inTraining,]
testing  <- spotiNormalizado[-inTraining,]

training2 <- spotifyNormNoMovements[ inTraining,]
testing2 <- spotifyNormNoMovements [-inTraining,]


# Definir el control del modelo
fitControl <- trainControl(
  method = "cv",  # Cross-validation
  number = 10,     # Número de folds
)
```

#### DropOut

Modelo que contiene `daily_rank`, `daily_movement`, `weekly_movement`.  
```{r, eval=FALSE}
modelo <- train(
  popularity ~ ., 
  data = training,
  method = "mlpKerasDropout",  
  trControl = fitControl,
  tuneGrid = gridDropOut,
  epochs = 25
)

saveRDS(object = modelo, file = "mlpNorm2Dropout.rds")
```

```{r}
mlpNorm2Dropout_model = readRDS("mlpDropout/Regresion/mlpNorm2Dropout.rds")
```

Modelo que no contiene `daily_rank`, `daily_movement`, `weekly_movement`. 
```{r, eval=FALSE}

modelo <- train(
  popularity ~ ., 
  data = training2,
  method = "mlpKerasDropout", 
  trControl = fitControl,
  tuneGrid = gridDropOut,
  epochs = 25
)

saveRDS(object = modelo, file = "mlpNormNoMov2Dropout.rds")
```

```{r}
mlpNormNoMov2Dropout_model = readRDS("mlpDropout/Regresion/mlpNormNoMov2Dropout.rds")
```

```{r}
mlpNorm2Dropout_model = calcular_metricas(mlpNorm2Dropout_model, testing)
mlpNormNoMov2Dropout_model = calcular_metricas(mlpNormNoMov2Dropout_model, testing2)
```


#### Decay
Modelo que contiene `daily_rank`, `daily_movement`, `weekly_movement`. 
```{r, eval=FALSE}

# Entrenar el modelo utilizando train()
modelo <- train(
  popularity ~ ., 
  data = training,
  method = "mlpKerasDecay", 
  trControl = fitControl,
  tuneGrid = gridDecay,
  epochs = 25
)

saveRDS(object = modelo, file = "mlpNorm2Decay.rds")
```

```{r}
mlpNorm2Decay_model = readRDS("mlpDecay/Regresion/mlpNorm2Decay.rds")
predicciones2 = mlpNorm2Decay_model
```

Modelo que no contiene `daily_rank`, `daily_movement`, `weekly_movement`. 
```{r, eval=FALSE}
modelo <- train(
  popularity ~ ., 
  data = training2,
  method = "mlpKerasDecay",  
  trControl = fitControl,
  tuneGrid = gridDecay,
  epochs = 25
)
saveRDS(object = modelo, file = "mlpNormNoMov2Decay.rds")
```


```{r}
mlpNormNoMov2Decay_model = readRDS("mlpDecay/Regresion/mlpNormNoMov2Decay.rds")
```

```{r}
mlpNorm2Decay_model = calcular_metricas(mlpNorm2Decay_model,testing)
mlpNormNoMov2Decay_model = calcular_metricas(mlpNormNoMov2Decay_model, testing2)
```

### Resultados
**Describir los resultados del algoritmo.**
Ya tenemos todos los modelos entrenados, a continuación, lo que vamos a hacer es ver cuales son los resultados obtenidos, y comprobar si dan mejores resultados utilizando Dropout o Decay, y si son los modelos con `"movement"` mejores que los que no lo tienen.

```{r}
comparacion = data.frame(
  mlpNorm1Dropout,
  mlpNormNoMovDropout_model,
  mlpNorm1Decay_model,
  mlpNormNoMovDecay_model
)
comparacion
```
Al contrario que en arboles, utilizando min-max, podemos ver que aquí si que se tiene en cuenta la posición en el ranking y los movimientos. Con el modelo con el que mejores resultados obtenemos es con el mlpNorm1Decay_model. Por lo que, conseguimos que nos funcionen mejor las redes utilizando Decay que Dropout.

A continuación vamos a ver si los resultados se mantienen para el escalado z-score.

```{r}
comparacion = data.frame(
  mlpNorm2Dropout_model,
  mlpNormNoMov2Dropout_model,
  mlpNorm2Decay_model,
  mlpNormNoMov2Decay_model
)
comparacion
```

Pues los resultados obtenidos anteriormente tambien se mantienen. El mejor modelo es el obtenido con Decay y que contiene los parámetros de ranking y movimientos en la lista.

Finalmente vamos a analizar los dos mejores modelos de mlp

```{r}
comparacion = data.frame(
  mlpNorm1Decay_model,
  mlpNorm2Decay_model
)
comparacion
```
MAE: En este caso, el modelo mlpNorm1Decay_model tiene un MAE más bajo (5.867074), lo que indica que, en promedio, las predicciones están más cerca de los valores reales en comparación con mlpNorm2Decay_model (6.416891). Un MAE más bajo sugiere un mejor rendimiento en términos de precisión.

RMSE: Al igual que el MAE, el valor más bajo de RMSE en mlpNorm1Decay_model (9.227483) indica que este modelo tiene un rendimiento ligeramente mejor en términos de ajuste a los datos en comparación con mlpNorm2Decay_model (9.586898).

En términos de ambos MAE y RMSE, mlpNorm1Decay_model parece tener un mejor rendimiento que mlpNorm2Decay_model. Esto sugiere que el primer modelo tiene una capacidad predictiva superior en este conjunto de datos específico.
Aún así creemos que no es el mejor modelo después de haber visto los resultados de los arboles, de esto hablaremos en el ultimo apartado.

Pero podemos ver un plot con las predicciones de ambos y el valor real y ver que son casi iguales entre sí, aunque la prediccion, como podemos ver no es la mejor. 

```{r}
predicciones1 = predict(predicciones1, newdata = sptesting1)
datos = data.frame(Predicciones = predicciones1, Popularidad = sptesting1$popularity)

ggplot(datos, aes(x = Predicciones, y = Popularidad)) +
  geom_point(aes(color = "Predicciones"), size = 1, alpha = 0.2) +
  geom_point(aes(x = Popularidad, color = "Popularidad"), size = 1, alpha = 0.3) +
  labs(title = "Comparación de Predicciones y Popularidad",
       x = "Predicciones", y = "Popularidad") +
  theme_minimal() +
  scale_color_manual(name = "Conjunto",
                     values = c("Predicciones" = "violet", "Popularidad" = "red"))
```

```{r}
predicciones2 = predict(predicciones2, newdata = testing)
datos = data.frame(Predicciones = predicciones1, Popularidad = sptesting1$popularity)

ggplot(datos, aes(x = Predicciones, y = Popularidad)) +
  geom_point(aes(color = "Predicciones"), size = 1, alpha = 0.2) +
  geom_point(aes(x = Popularidad, color = "Popularidad"), size = 1, alpha = 0.3) +
  labs(title = "Comparación de Predicciones y Popularidad",
       x = "Predicciones", y = "Popularidad") +
  theme_minimal() +
  scale_color_manual(name = "Conjunto",
                     values = c("Predicciones" = "violet", "Popularidad" = "red"))
```

## Clustering

En este apartado además se incluyen apartados de clasificación. Se abordará la elección del mejor modelo de clasificación en el ultimo apartado extra que hemos añadido.

### Funcionamiento
**Explicar brevemente el tipo de modelo que genera el algoritmo, y cuál es la estrategia de dicho algoritmos para construir el modelo.**

El algoritmo de clustering se utiliza para agrupar datos similares entre sí en conjuntos llamados "clusters". Uno de los métodos más comunes es el k-means. Su estrategia es la siguiente:

1. **Inicialización:** Se seleccionan aleatoriamente los centroides iniciales para los k clusters.

2. **Asignación:** Cada punto de datos se asigna al cluster cuyo centroide es el más cercano.

3. **Actualización:** Se recalculan los centroides de los clusters basándose en los puntos de datos asignados.

4. **Iteración:** Se repiten los pasos 2 y 3 hasta que los centroides convergen o se alcanza un número máximo de iteraciones.

El resultado es un conjunto de clusters donde los puntos de datos dentro de cada cluster son más similares entre sí que con aquellos en otros clusters.

Como hemos mencionado, uno de sus métodos más conocidos es `knn`. Lo que hace este método es buscar en el conjunto de entrenamiento K muestras que sean las más cercanas en distancia al nuevo punto que se desea clasificar. La distancia puede medirse utilizando diversas métricas, siendo la distancia euclidiana la opción común. Después de identificar estos K vecinos más cercanos, el algoritmo predice la etiqueta del nuevo punto basándose en la mayoría de las etiquetas de estas muestras de entrenamiento cercanas. 

KNN clasifica un nuevo punto según la mayoría de las clases de sus K vecinos más cercanos en el espacio de características.


### Requisitos
**Indicar si el algoritmo en cuestión tiene algún requisito en cuanto a si se han de preprocesar los datos (e.g. escalado, imputación de valores nulos, etc.) y cómo. Explicar cómo se ha tenido en cuenta estos requisitos a la hora de generar los datos de training específicos para este algoritmo.**

Los requisitos para el clustering, especialmente para k-means, incluyen:

- Escalado de variables: Dado que el algoritmo de k-means se basa en distancias euclidianas, es importante escalar las variables para que tengan la misma influencia en el cálculo de las distancias. Esto ya lo hemos contemplado en la preparación de los datos.

- Manejo de valores nulos: Los valores nulos pueden afectar la calidad del clustering, por lo que es importante tratarlos antes del entrenamiento, lo cual hemos ya.

### Descripción de hiperparámetros
**Identificar y explicar cada uno de sus parámetros de configuración.**
Los hiperparámetros de configuración son los siguientes:

```{r}
knnInfo = getModelInfo("knn")
knnInfo = knnInfo$knn
knnInfo$parameters
```

Vemos que solo tenemos un parámetro, **k**, que es el número de vecinos más cercanos que se tienen en cuenta al realizar una predicción.

```{r}
spotiNormalizado = spSpotiNormalizado
spotifyNormNoMovements = spSpotifyNormNoMovements
spotiNormalizadoZScore = spSpotiNormalizadoZScore
spotifyNormNoMovementsZScore = spSpotifyNormNoMovementsZScore
```

### Grid Hiperparámetros

**Detallar una estrategia para la generación del grid de valores para hiperparámetros a usar.**

El único hiperparámetro al que le vamos a asignar distintos valores es k. En un principio pusimos valores pequeños como 5, 6, 7 pero vimos que los resultados eran pésimos así que decidimos poner valores más altos y des esta forma tuvimos mejores resultados como se va a ver a continuación.

```{r knnReg1, eval=TRUE}
set.seed(123)

# Definir las variables de entrada y salida
spotiKNN.Var.Salida.Usada <- c("popularity")
spotiKNN.Vars.Entrada.Usadas <- setdiff(names(spotiNormalizado), spotiKNN.Var.Salida.Usada)

# Crear una partición del 80% para entrenamiento y 20% para prueba
spotiKNN.Particion <- createDataPartition(spotiNormalizado[[spotiKNN.Var.Salida.Usada]], 
                                          p = 0.8, 
                                          list = FALSE,
                                          times = 1)

# Crear conjuntos de entrenamiento y prueba
spotiKNN.Datos.Entrenamiento <- spotiNormalizado[spotiKNN.Particion, ]
spotiKNN.Datos.Prueba <- spotiNormalizado[-spotiKNN.Particion, ]

# Usamos cross-validación 
fitControl <- trainControl(## 10-fold CV
  method = "cv",
  number = 10)
mygrid <- expand.grid(
  k = c(10, 50, 100, 200))
```

```{r knnReg2, eval=FALSE}
# Crear modelos utilizando el método train() de Caret
spotiKNN.modelo.knnReg <- train(popularity ~ ., 
                                data = spotiKNN.Datos.Entrenamiento, 
                                method = "knn", 
                                trControl = fitControl, 
                                tuneGrid = mygrid)
spotiKNN.modelo.knnReg
saveRDS(spotiKNN.modelo.knnReg, "knnReg.rds")
```

```{r knnReg3, eval=TRUE}
knnReg_model = readRDS("knn/knnReg.rds")
knnReg_model

set.seed(123)  # Establecer semilla para reproducibilidad
predicciones <- predict(knnReg_model, newdata = spotiKNN.Datos.Prueba)
maeKnnReg <- MAE(predicciones, spotiKNN.Datos.Prueba$popularity)
rmseKnnReg <- RMSE(predicciones, spotiKNN.Datos.Prueba$popularity)

maeKnnReg; rmseKnnReg
```

Ahora vamos a probar con los datos normalizados pero sin las variables `daily_movement`, `daily_rank` y `weekly_movement`.

```{r knnRegNoMov1, eval=TRUE}
set.seed(123)

# Definir las variables de entrada y salida
spotiKNN.Var.Salida.Usada <- c("popularity")
spotiKNN.Vars.Entrada.Usadas <- setdiff(names(spotifyNormNoMovements), spotiKNN.Var.Salida.Usada)

# Crear una partición del 80% para entrenamiento y 20% para prueba
spotiKNN.Particion <- createDataPartition(spotifyNormNoMovements[[spotiKNN.Var.Salida.Usada]], 
                                          p = 0.8, 
                                          list = FALSE,
                                          times = 1)

# Crear conjuntos de entrenamiento y prueba
spotiKNN.Datos.Entrenamiento <- spotifyNormNoMovements[spotiKNN.Particion, ]
spotiKNN.Datos.Prueba <- spotifyNormNoMovements[-spotiKNN.Particion, ]

# Usamos cross-validación 
fitControl <- trainControl(## 10-fold CV
  method = "cv",
  number = 10)
mygrid <- expand.grid(
  k = c(10, 50, 100, 200))
```

```{r knnRegNoMov2, eval=FALSE}
# Crear modelos utilizando el método train() de Caret
spotiKNN.modelo.knnRegNoMov <- train(popularity ~ ., 
                                     data = spotiKNN.Datos.Entrenamiento, 
                                     method = "knn", 
                                     trControl = fitControl, 
                                     tuneGrid = mygrid)
spotiKNN.modelo.knnRegNoMov
saveRDS(spotiKNN.modelo.knnRegNoMov, "knn/knnRegNoMov.rds")
```

```{r knnRegNoMov3, eval=TRUE}
set.seed(123)
knnRegNoMov_model = readRDS("knn/knnRegNoMov.rds")
knnRegNoMov_model

predicciones <- predict(knnRegNoMov_model, newdata = spotiKNN.Datos.Prueba)
maeKnnRegNoMov <- MAE(predicciones, spotiKNN.Datos.Prueba$popularity)
rmseKnnRegNoMov <- RMSE(predicciones, spotiKNN.Datos.Prueba$popularity)

maeKnnRegNoMov; rmseKnnRegNoMov
```

Y a continuación con clasificación: 
```{r knnCla1, eval=TRUE}
spotiNormalizadoCL <- spotiNormalizado
spotiNormalizadoCL$popularity_group <- cut(spotiNormalizado$popularity, 
                                           breaks = c(0, 30, 60, 90, 101), 
                                           labels = c("nada popular", "poco popular", "algo popular", "hit"),
                                           include.lowest = TRUE, 
                                           right = FALSE)
spotiNormalizadoCL <- spotiNormalizadoCL[, !names(spotiNormalizadoCL) %in% c("popularity")]

set.seed(123)

# Definir las variables de entrada y salida
spotiKNN.Var.Salida.Usada <- c("popularity_group")
spotiKNN.Vars.Entrada.Usadas <- setdiff(names(spotiNormalizadoCL), spotiKNN.Var.Salida.Usada)

# Crear una partición del 80% para entrenamiento y 20% para prueba
spotiKNN.Particion <- createDataPartition(spotiNormalizadoCL[[spotiKNN.Var.Salida.Usada]], 
                                          p = 0.8, 
                                          list = FALSE,
                                          times = 1)

# Crear conjuntos de entrenamiento y prueba
spotiKNN.Datos.Entrenamiento <- spotiNormalizadoCL[spotiKNN.Particion, ]
spotiKNN.Datos.Prueba <- spotiNormalizadoCL[-spotiKNN.Particion, ]
```
```{r knnCla2, eval=FALSE}
# Crear modelos utilizando el método train() de Caret
spotiKNN.modelo.knnCla <- train(popularity_group ~ ., 
                                data = spotiKNN.Datos.Entrenamiento, 
                                method = "knn", 
                                trControl = fitControl, 
                                tuneGrid = mygrid)

saveRDS(spotiKNN.modelo.knnCla, "knnCla.rds")
```

```{r knnCla3, eval=TRUE}
knnCla_model = readRDS("knn/knnCla.rds")
knnCla_model

predicciones <- predict(knnCla_model, newdata = spotiKNN.Datos.Prueba)
# Calcular matriz de confusión
cmknnCla <- confusionMatrix(predicciones, spotiKNN.Datos.Prueba$popularity_group)
cmknnCla
```
Clasificación con alta precisión en las categorías "algo popular" y "hit". Buen rendimiento general con un **92.24%** de precisión, indicando una sólida capacidad para predecir la popularidad de las canciones.

```{r knnClaNoMov1, eval=TRUE}
spotiNormalizadoNoMovCL <- spotifyNormNoMovements
spotiNormalizadoNoMovCL$popularity_group <- cut(spotifyNormNoMovements$popularity, 
                                                breaks = c(0, 30, 60, 90, 101), 
                                                labels = c("nada popular", "poco popular", "algo popular", "hit"),
                                                include.lowest = TRUE, 
                                                right = FALSE)
spotiNormalizadoNoMovCL <- spotiNormalizadoNoMovCL[, !names(spotiNormalizadoNoMovCL) %in% c("popularity")]

set.seed(123)

# Definir las variables de entrada y salida
spotiKNN.Var.Salida.Usada <- c("popularity_group")
spotiKNN.Vars.Entrada.Usadas <- setdiff(names(spotiNormalizadoNoMovCL), spotiKNN.Var.Salida.Usada)

# Crear una partición del 80% para entrenamiento y 20% para prueba
spotiKNN.Particion <- createDataPartition(spotiNormalizadoNoMovCL[[spotiKNN.Var.Salida.Usada]], 
                                          p = 0.8, 
                                          list = FALSE,
                                          times = 1)

# Crear conjuntos de entrenamiento y prueba
spotiKNN.Datos.Entrenamiento <- spotiNormalizadoNoMovCL[spotiKNN.Particion, ]
spotiKNN.Datos.Prueba <- spotiNormalizadoNoMovCL[-spotiKNN.Particion, ]
```

```{r knnClaNoMov2, eval=FALSE}
# Crear modelos utilizando el método train() de Caret
spotiKNN.modelo.knnClaNoMov <- train(popularity_group ~ ., 
                                     data = spotiKNN.Datos.Entrenamiento, 
                                     method = "knn", 
                                     trControl = fitControl, 
                                     tuneGrid = mygrid)

saveRDS(spotiKNN.modelo.knnClaNoMov, "knnClaNoMov.rds")
```

```{r knnClaNoMov3}
knnClaNoMov_model = readRDS("knn/knnClaNoMov.rds")
knnClaNoMov_model

predicciones <- predict(knnClaNoMov_model, newdata = spotiKNN.Datos.Prueba)
# Calcular matriz de confusión
cmknnClaNoMov <- confusionMatrix(predicciones, spotiKNN.Datos.Prueba$popularity)
cmknnClaNoMov
```

El modelo de clasificación presenta un rendimiento sólido, logrando una precisión global del **91.25%**. Destaca en la predicción de categorías como "poco popular", "algo popular" y "hit", con sensibilidades superiores al 72%.

#### Normalizado z-score

Por otra parte, vamos a ver qué tal serían los resultados para el normalizado z-score. Primero sin quitar las 3 columnas de antes.
```{r knnRegZ1, eval=TRUE}
set.seed(123)

# Definir las variables de entrada y salida
spotiKNN.Var.Salida.Usada <- c("popularity")
spotiKNN.Vars.Entrada.Usadas <- setdiff(names(spotiNormalizadoZScore), spotiKNN.Var.Salida.Usada)

# Crear una partición del 80% para entrenamiento y 20% para prueba
spotiKNN.Particion <- createDataPartition(spotiNormalizadoZScore[[spotiKNN.Var.Salida.Usada]], 
                                          p = 0.8, 
                                          list = FALSE,
                                          times = 1)

# Crear conjuntos de entrenamiento y prueba
spotiKNN.Datos.Entrenamiento <- spotiNormalizadoZScore[spotiKNN.Particion, ]
spotiKNN.Datos.Prueba <- spotiNormalizadoZScore[-spotiKNN.Particion, ]

# Usamos cross-validación 
fitControl <- trainControl(## 10-fold CV
  method = "cv",
  number = 10)
mygrid <- expand.grid(
  k = c(10, 50, 100, 200))
```


```{r knnRegZ2, eval=FALSE}
# Crear modelos utilizando el método train() de Caret
spotiKNN.modelo.knnRegZ <- train(popularity ~ ., 
                                 data = spotiKNN.Datos.Entrenamiento, 
                                 method = "knn", 
                                 trControl = fitControl, 
                                 tuneGrid = mygrid)
spotiKNN.modelo.knnRegZ
saveRDS(spotiKNN.modelo.knnRegZ, "knnRegZ.rds")
```

```{r knnRegZ3, eval =TRUE }
knnRegZ_model = readRDS("knn/knnRegZ.rds")

predicciones <- predict(knnRegZ_model, newdata = spotiKNN.Datos.Prueba)

maeKnnRegZ <- MAE(predicciones, spotiKNN.Datos.Prueba$popularity)
rmseKnnRegZ <- RMSE(predicciones, spotiKNN.Datos.Prueba$popularity)

maeKnnRegZ; rmseKnnRegZ
```

Y ahora para el dataset sin las 3 columnas y con z-score.
```{r knnRegNoMovZ1, eval=TRUE}
set.seed(123)

# Definir las variables de entrada y salida
spotiKNN.Var.Salida.Usada <- c("popularity")
spotiKNN.Vars.Entrada.Usadas <- setdiff(names(spotifyNormNoMovementsZScore), spotiKNN.Var.Salida.Usada)

# Crear una partición del 80% para entrenamiento y 20% para prueba
spotiKNN.Particion <- createDataPartition(spotifyNormNoMovementsZScore[[spotiKNN.Var.Salida.Usada]], 
                                          p = 0.8, 
                                          list = FALSE,
                                          times = 1)

# Crear conjuntos de entrenamiento y prueba
spotiKNN.Datos.Entrenamiento <- spotifyNormNoMovementsZScore[spotiKNN.Particion, ]
spotiKNN.Datos.Prueba <- spotifyNormNoMovementsZScore[-spotiKNN.Particion, ]

# Usamos cross-validación 
fitControl <- trainControl(## 10-fold CV
  method = "cv",
  number = 10)
mygrid <- expand.grid(
  k = c(10, 50, 100, 200))
```

```{r knnRegNoMovZ2, eval=FALSE}
# Crear modelos utilizando el método train() de Caret
spotiKNN.modelo.knnRegNoMovZ <- train(popularity ~ ., 
                                      data = spotiKNN.Datos.Entrenamiento, 
                                      method = "knn", 
                                      trControl = fitControl, 
                                      tuneGrid = mygrid)
spotiKNN.modelo.knnRegNoMovZ
saveRDS(spotiKNN.modelo.knnRegNoMovZ, "knnRegNoMovZ.rds")
```

```{r knnRegNoMovZ3, eval =TRUE}
knnRegNoMovZ_model = readRDS("knn/knnRegNoMovZ.rds")
knnRegNoMovZ_model

predicciones <- predict(knnRegNoMovZ_model, newdata = spotiKNN.Datos.Prueba)
maeKnnRegNoMovZ <- MAE(predicciones, spotiKNN.Datos.Prueba$popularity)
rmseKnnRegNoMovZ <- RMSE(predicciones, spotiKNN.Datos.Prueba$popularity)

maeKnnRegNoMovZ; rmseKnnRegNoMovZ
```


Y ahora quitando las 3 columnas.
```{r knnClaZ1, eval=TRUE}
spotiNormalizadoCL <- spotiNormalizadoZScore
spotiNormalizadoCL$popularity_group <- cut(spotiNormalizadoZScore$popularity, 
                                           breaks = c(0, 30, 60, 90, 101), 
                                           labels = c("nada popular", "poco popular", "algo popular", "hit"),
                                           include.lowest = TRUE, 
                                           right = FALSE)
spotiNormalizadoCL <- spotiNormalizadoCL[, !names(spotiNormalizadoCL) %in% c("popularity")]

set.seed(123)

# Definir las variables de entrada y salida
spotiKNN.Var.Salida.Usada <- c("popularity_group")
spotiKNN.Vars.Entrada.Usadas <- setdiff(names(spotiNormalizadoCL), spotiKNN.Var.Salida.Usada)

# Crear una partición del 80% para entrenamiento y 20% para prueba
spotiKNN.Particion <- createDataPartition(spotiNormalizadoCL[[spotiKNN.Var.Salida.Usada]], 
                                          p = 0.8, 
                                          list = FALSE,
                                          times = 1)

# Crear conjuntos de entrenamiento y prueba
spotiKNN.Datos.Entrenamiento <- spotiNormalizadoCL[spotiKNN.Particion, ]
spotiKNN.Datos.Prueba <- spotiNormalizadoCL[-spotiKNN.Particion, ]
```

```{r knnClaZ2, eval=FALSE}
# Crear modelos utilizando el método train() de Caret
spotiKNN.modelo.knnClaZ <- train(popularity_group ~ ., 
                                 data = spotiKNN.Datos.Entrenamiento, 
                                 method = "knn", 
                                 trControl = fitControl, 
                                 tuneGrid = mygrid)

saveRDS(spotiKNN.modelo.knnClaZ, "knnClaZ.rds")
```

```{r knnClaZ3, eval =TRUE}
knnClaZ_model = readRDS("knn/knnClaZ.rds")
knnClaZ_model

predicciones <- predict(knnClaZ_model, newdata = spotiKNN.Datos.Prueba)
# Calcular matriz de confusión
cmknnClaZ <- confusionMatrix(predicciones, spotiKNN.Datos.Prueba$popularity_group)
cmknnClaZ
```

El modelo de clasificación exhibe un rendimiento sólido con una precisión global del **91.72%**. Destaca en la predicción de las categorías "poco popular", "algo popular" y "hit" con sensibilidades superiores al 70%, indicando una buena capacidad para identificar estas categorías. 

```{r knnClaNoMovZ1, eval=TRUE}
spotiNormalizadoNoMovZ_CL <- spotifyNormNoMovementsZScore
spotiNormalizadoNoMovZ_CL$popularity_group <- cut(spotifyNormNoMovementsZScore$popularity, 
                                                  breaks = c(0, 30, 60, 90, 101), 
                                                  labels = c("nada popular", "poco popular", "algo popular", "hit"),
                                                  include.lowest = TRUE, 
                                                  right = FALSE)
spotiNormalizadoNoMovZ_CL <- spotiNormalizadoNoMovZ_CL[, !names(spotiNormalizadoNoMovZ_CL) %in% c("popularity")]

set.seed(123)

# Definir las variables de entrada y salida
spotiKNN.Var.Salida.Usada <- c("popularity_group")
spotiKNN.Vars.Entrada.Usadas <- setdiff(names(spotiNormalizadoNoMovZ_CL), spotiKNN.Var.Salida.Usada)

# Crear una partición del 80% para entrenamiento y 20% para prueba
spotiKNN.Particion <- createDataPartition(spotiNormalizadoNoMovZ_CL[[spotiKNN.Var.Salida.Usada]], 
                                          p = 0.8, 
                                          list = FALSE,
                                          times = 1)

# Crear conjuntos de entrenamiento y prueba
spotiKNN.Datos.Entrenamiento <- spotiNormalizadoNoMovZ_CL[spotiKNN.Particion, ]
spotiKNN.Datos.Prueba <- spotiNormalizadoNoMovZ_CL[-spotiKNN.Particion, ]
```

```{r knnClaNoMovZ2, eval=FALSE}
# Crear modelos utilizando el método train() de Caret
spotiKNN.modelo.knnClaNoMovZ <- train(popularity_group ~ ., 
                                      data = spotiKNN.Datos.Entrenamiento, 
                                      method = "knn", 
                                      trControl = fitControl, 
                                      tuneGrid = mygrid)

saveRDS(spotiKNN.modelo.knnClaNoMovZ, "knnClaNoMovZ.rds")
```

```{r knnClaNoMovZ3, eval =TRUE}
knnClaNoMovZ_model = readRDS("knn/knnClaNoMovZ.rds")
knnClaNoMovZ_model

predicciones <- predict(knnClaNoMovZ_model, newdata = spotiKNN.Datos.Prueba)
# Calcular matriz de confusión
cmknnClaNoMovZ <- confusionMatrix(predicciones, spotiKNN.Datos.Prueba$popularity_group)
cmknnClaNoMovZ
```

El modelo de clasificación presenta una precisión global del **91.09%**, demostrando un rendimiento consistente en la categorización de la popularidad de canciones. Aunque la sensibilidad en la categoría "nada popular" es baja, las sensibilidades en "poco popular", "algo popular" y "hit" son superiores al 72%


### Resultados
**Describir los resultados del algoritmo.**

Ahora vamos a recoger en una tabla para **regresión** los resultados del MAE y RMSE para compararlos y comentarlos.

```{r comparation, eval=TRUE}
comparacion <- data.frame(
  Modelo = c("KnnReg", "KnnRegNoMov", "KnnRegZ", "KnnRegNoMovZ"),
  MAE = c(maeKnnReg, maeKnnRegNoMov, maeKnnRegZ, maeKnnRegNoMovZ),
  RMSE = c(rmseKnnReg, rmseKnnRegNoMov, rmseKnnRegZ, rmseKnnRegNoMovZ)
)
comparacion
```

El modelo KnnReg parece tener un buen rendimiento en términos de error absoluto medio (MAE) y raíz del error cuadrático medio (RMSE). Un MAE de 2.99 sugiere que, en promedio, las predicciones están desviadas por aproximadamente 2.99 unidades de la variable objetivo (popularidad). El RMSE de 8.06 indica la dispersión de los errores, siendo más sensible a los errores grandes.

El KnnRegNoMov, que excluye las variables `daily_movement`, `daily_rank` y `weekly_movement`, muestra un ligero aumento en ambos MAE y RMSE en comparación con el modelo anterior. Esto podría indicar que esas variables pueden contener información útil para las predicciones.

Por otra parte, KnnRegZ es un modelo normalizado z-score que parece tener un rendimiento similar al modelo original por lo que tal vez esta normalización es mejor para el modelo.

Por último, el modelo KnnRegNoMovZ es similar al modelo KnnRegNoMov. La versión normalizada muestra un aumento en MAE y RMSE en comparación con su contraparte con las variables. Esto respalda la idea de que las variables pueden ser cruciales.

### Resultados
**Describir los resultados del algoritmo.**
Por lo que el mejor modelo es KnnRegZ, normalizado con z-score y con las variables.

# Apartado D
## Modelo Final

```{r, eval=TRUE}
set.seed(1212)

# Modelos preentrenados
modelo_rf <- rfNoMov
modelo_mlp <- mlpNorm1DecayModel
modelo_knn <- knnRegZ_model

# Recopilar Resamples
conjunto_resamples <- resamples(list(RandomForest = modelo_rf, 
                                     MLP = modelo_mlp, 
                                     KNN = modelo_knn))

# Visualización y Comparación
trellis.par.set(caretTheme())
bwplot(conjunto_resamples, layout = c(nrow = 3, ncol = 1))  
# Ajusta el diseño según la cantidad de modelos y métricas

# Pruebas de Hipótesis
diferencias <- diff(conjunto_resamples)
resumen_diferencias <- summary(diferencias)
```

El gráfico de caja compara las distribuciones de rendimiento de los modelos. 

Observamos que el modelo RandomForest tiene una mediana más alta en comparación con los otros dos. MLP también muestra un buen rendimiento, con una dispersión moderada. Y KNN, aunque presenta una variabilidad significativa, parece tener un rendimiento generalmente aceptable.

Estos resultados nos dan una idea visual de cómo se comparan los modelos en varias métricas y nos ayudan a tomar decisiones informadas.

Finalmente y como hemos mencionado de forma anterior el resultado escogido es el de random forest debido a que es el que menos error tiene en la regresión. 

# Apartado Extra
## Clasificación 
A continuación no vamos a detallar ni explicar tanto lo que hemos hecho, solo cargaremos algunos modelos de clasificación realizados porque pensamos que esta practica podría tener otra solución : determinar si una canción se vuelve o no un hit. Para ello hemos dividido la popularidad en distintos grupos, a continuación vamos a poner el codigo que hemos utilizado para realizar este apartado y remarcaremos las principales diferencias, junto con una explicación de los resultados.

Primero Dividimos la popularity en todos nuestros intervalos
```{r}
intervalos <- c(0, 30, 60, 90, 101)
spotify$popularity <- cut(spotify$popularity, breaks = intervalos, labels = c("nada popular", "poco popular", "algo popular", "hit"), include.lowest = TRUE, right = FALSE)

spotifyNoMovements$popularity <- cut(spotifyNoMovements$popularity, breaks = intervalos, labels = c("nada popular", "poco popular", "algo popular", "hit"), include.lowest = TRUE, right = FALSE)

spotiNormalizadoZScore$popularity <- cut(spotiNormalizadoZScore$popularity, breaks = intervalos, labels = c("nada popular", "poco popular", "algo popular", "hit"), include.lowest = TRUE, right = FALSE)

spotifyNormNoMovementsZScore$popularity <- cut(spotifyNormNoMovementsZScore$popularity, breaks = intervalos, labels = c("nada popular", "poco popular", "algo popular", "hit"), include.lowest = TRUE, right = FALSE)

spotiNormalizado$popularity <- cut(spotiNormalizado$popularity, breaks = intervalos, labels = c("nada popular", "poco popular", "algo popular", "hit"), include.lowest = TRUE, right = FALSE)

spotifyNormNoMovements$popularity <- cut(spotifyNormNoMovements$popularity, breaks = intervalos, labels = c("nada popular", "poco popular", "algo popular", "hit"), include.lowest = TRUE, right = FALSE)

```

## Random Forest

```{r}
set.seed(123)
inTraining <- createDataPartition(spotify$popularity, times=1,p = .80, list = FALSE)
training <- spotify[ inTraining,]
testing  <- spotify[-inTraining,]
```


```{r rf200trees2, eval=FALSE}
mygrid <- expand.grid(
  mtry = c(4, 5, 6),
  splitrule = c("gini", "extratrees"),
  min.node.size = c(50, 100, 150))

rf_200t <- train(popularity ~ ., data = training, 
                 method = "ranger", 
                 trControl = fitControl,
                 num.trees = 200,
                 tuneGrid=mygrid)

saveRDS(object= rf_200t, file = "classification200.rds")
```

```{r rf300trees2, eval=FALSE}
rf_300t <- train(popularity ~ ., data = training, 
                 method = "ranger", 
                 trControl = fitControl,
                 num.trees = 300,
                 tuneGrid=mygrid)

saveRDS(object= rf_300t, file = "classification300.rds")
```

```{r rf400trees2, eval=FALSE}
rf_400t <- train(popularity ~ ., data = training, 
                 method = "ranger", 
                 trControl = fitControl,
                 num.trees = 400,
                 tuneGrid=mygrid)

saveRDS(object= rf_400t, file = "classification400.rds")
```

```{r rf500trees2, eval=FALSE}
rf_500t <- train(popularity ~ ., data = training, 
                 method = "ranger", 
                 trControl = fitControl,
                 num.trees = 500,
                 tuneGrid=mygrid)

saveRDS(object= rf_500t, file = "classification500.rds")
```

```{r, eval=TRUE}
rf_200t = readRDS("rf/Clasificacion/classification200.rds")
rf_300t = readRDS("rf/Clasificacion/classification300.rds")
rf_400t = readRDS("rf/Clasificacion/classification400.rds")
rf_500t = readRDS("rf/Clasificacion/classification500.rds")

resultados <- data.frame(
  "NumArboles" = c("200", "300", "400", "500"),
  "Splitrule" = c(rf_200t$bestTune$splitrule, rf_300t$bestTune$splitrule, rf_400t$bestTune$splitrule, rf_500t$bestTune$splitrule),
  "Min node size" = c(rf_200t$bestTune$min.node.size, rf_300t$bestTune$min.node.size, rf_400t$bestTune$min.node.size, rf_500t$bestTune$min.node.size),
  "Mtry" = c(rf_200t$bestTune$mtry, rf_300t$bestTune$mtry, rf_400t$bestTune$mtry, rf_500t$bestTune$mtry)
)
resultados
```

Podemos para todas las opciones de número de árboles que hemos contemplado obtenemos casi la misma configuración, solo varía Mtry. Por lo que para el dataset sin ranking y movimientos vamos a entrenar variando solo este parametro

```{r, eval=TRUE}
calcular_metricas <- function(modelo) {
  predicciones <- predict(modelo, newdata = testing)
  matriz <- confusionMatrix(predicciones, testing$popularity)
  return(matriz)
}

rf_200tm = calcular_metricas(rf_200t)
rf_300tm = calcular_metricas(rf_300t)
rf_400tm = calcular_metricas(rf_400t)
rf_500tm = calcular_metricas(rf_500t)

#rf_200tm; rf_300tm; rf_400tm; rf_500tm

rf_500tm

```
La precisión es super alta. Sabemos que se debe a la clasificación y hemos conseguido esta clasificación tan alta debido a la secuencialidad de datos. Los mejores resultados son para 500 arboles. Por lo que como hemos estado haciendo calculamos el siguiente dataset de NoMovement con 500 arboles


Para este dataset, en lugar de ejecutarlo con distintas posibilidades, como en todos los anteriores nos ha dado la misma combinación, para este solo vamos a utilizar esa en concreto con un número de 500 arboles. Recordamos que este dataset, no tiene los atributos de `daily_rank`, `daily_movement`, `weekly_movement`.

```{r}
set.seed(123)
training <- spotifyNoMovements[ inTraining,]
testing <- spotifyNoMovements[ -inTraining, ]
fitControl <- trainControl(## 10-fold CV
  method = "cv",
  number = 10)

mygrid <- expand.grid(
  mtry = c(4,6),
  splitrule = c("variance"),
  min.node.size = 50)
mygrid

fitControl <- trainControl(## 10-fold CV
  method = "cv",
  number = 10)
```


```{r, eval =FALSE}
spotifyrf <-train (popularity ~ ., data = training,
                   method = "ranger",
                   trControl = fitControl,
                   num.trees = 500,
                   tuneGrid=mygrid)

saveRDS(object= spotifyrf, file = "spotifyNoMovementsClasificacion.rds")
```

```{r}
spotifyNoMovementsClasificacion = readRDS("rf/Clasificacion/spotifyNoMovementsClasificacion.rds")
spotifyNoMovementsClasificacion = calcular_metricas(spotifyNoMovementsClasificacion)
```


```{r}
spotifyNoMovementsClasificacion
```
Anteriormente en regresión, este modelo era el que menos error presentaba.Volvemos a repetir que estos modelos tienen una precisión tan alta debido a que por un lado solo tenemos 4 clases para clasificar (en lugar de una regresion de 100 numeros) y que de la forma que hemos aprovecha la secuencialidad, esta aporta muchisima informacion a los modelos.
## Deep Learning

Redefinimos la función ya que vamos a utilizar dos conjuntos de testing distintos.
```{r}
calcular_metricas <- function(modelo, test) {
  predicciones <- predict(modelo, newdata = test)
  matriz <- confusionMatrix(predicciones, testing$popularity)
  return(matriz)
}
```

Aqui estamos reusando algunos savePoints que hicimos anteriormente, ya que estas variables iban a sufrir modificaciones en la regresion.
```{r}
spotiNormalizado = spSpotiNormalizado2
spotifyNormNoMovements = spSpotifyNormNoMovements2 
spotiNormalizadoZScore= spSpotiNormalizadoZScore2  
spotifyNormNoMovementsZScore = spSpotifyNormNoMovementsZScore2 
intervalos <- c(0, 30, 60, 90, 101)
spotiNormalizadoZScore$popularity <- cut(spotiNormalizadoZScore$popularity, breaks = intervalos, labels = c("nada popular", "poco popular", "algo popular", "hit"), include.lowest = TRUE, right = FALSE)

spotifyNormNoMovementsZScore$popularity <- cut(spotifyNormNoMovementsZScore$popularity, breaks = intervalos, labels = c("nada popular", "poco popular", "algo popular", "hit"), include.lowest = TRUE, right = FALSE)

spotiNormalizado$popularity <- cut(spotiNormalizado$popularity, breaks = intervalos, labels = c("nada popular", "poco popular", "algo popular", "hit"), include.lowest = TRUE, right = FALSE)

spotifyNormNoMovements$popularity <- cut(spotifyNormNoMovements$popularity, breaks = intervalos, labels = c("nada popular", "poco popular", "algo popular", "hit"), include.lowest = TRUE, right = FALSE)

```



```{r}
set.seed(124)

inTraining <- createDataPartition(spotiNormalizado$popularity, times=1,p = .80, list = FALSE)
training <- spotiNormalizado[ inTraining,]
testing  <- spotiNormalizado[-inTraining,]

training2 <- spotifyNormNoMovements[ inTraining,]
testing2 <- spotifyNormNoMovements [-inTraining,]

gridDropOut <- expand.grid(
  size = 40, 
  dropout =0.1,
  batch_size = c(15,20),
  lr = c(0.01,0.02),
  rho = 0.91,
  decay = c(1e-6, 1e-5),
  activation = 'tanh'
)

gridDecay <- expand.grid(
  size = 40, 
  lambda =0.001,
  batch_size = c(15,20),
  lr = c(0.01,0.02),
  rho = 0.91,
  decay = c(1e-6, 1e-5),
  activation = 'tanh'
)

# Definir el control del modelo
fitControl <- trainControl(
  method = "cv",  # Cross-validation
  number = 10,     # Número de folds
)
```

### Dropout

```{r, eval=FALSE}

modelo <- train(
  popularity ~ ., 
  data = training,
  method = "mlpKerasDropout",  
  trControl = fitControl,
  tuneGrid = gridDropOut,
  epochs = 25
)


saveRDS(object = modelo, file = "mlpNorm1DropoutClasificacion.rds")
```

```{r}
mlpNorm1DropoutClasificacion = readRDS("mlpDropout/Clasificacion/mlpNorm1DropoutClasificacion.rds")
mlpfinal = mlpNorm1DropoutClasificacion
mlpNorm1DropoutClasificacion = calcular_metricas(mlpNorm1DropoutClasificacion, testing)
```


```{r, eval=FALSE}
modelo <- train(
  popularity ~ ., 
  data = training2,
  method = "mlpKerasDropout", 
  trControl = fitControl,
  tuneGrid = gridDropOut,
  epochs = 25
)

saveRDS(object = modelo, file = "mlpNorm1NoMovDropoutClasificacion.rds")
```

```{r}
mlpNorm1NoMovDropout_clasificacion = readRDS("mlpDropout/Clasificacion/mlpNorm1NoMovDropoutClasificacion.rds")
mlpNorm1NoMovDropout_clasificacion = calcular_metricas(mlpNorm1NoMovDropout_clasificacion, testing2)
```


### Decay

```{r, eval=FALSE}
modelo <- train(
  popularity ~ ., 
  data = training,
  method = "mlpKerasDecay", 
  trControl = fitControl,
  tuneGrid = gridDecay,
  epochs = 25
)

saveRDS(object = modelo, file = "mlpNorm1DecayClasificacion.rds")
```

```{r}
mlpNorm1DecayClasificacion = readRDS("mlpDecay/Clasificacion/mlpNorm1DecayClasificacion.rds")
mlpNorm1DecayClasificacion = calcular_metricas(mlpNorm1DecayClasificacion, testing)
```


```{r, eval=FALSE}
modelo <- train(
  popularity ~ ., 
  data = training2,
  method = "mlpKerasDecay",  # Puedes ajustar esto según el nombre del método que estás utilizando
  trControl = fitControl,
  tuneGrid = gridDecay,
  epochs = 25
)


saveRDS(object = modelo, file = "mlpNorm1NoMovDecayClasificacion.rds")
```

```{r}
mlpNorm1NoMovDecayClasificacion = readRDS("mlpDecay/Clasificacion/mlpNorm1NoMovDecayClasificacion.rds")
mlpNorm1NoMovDecayClasificacion = calcular_metricas(mlpNorm1NoMovDecayClasificacion, testing2)
```

### Normalizado z-score

```{r, eval=FALSE}
set.seed(125)
spotiNormalizado = spotiNormalizadoZScore
spotifyNormNoMovements = spotifyNormNoMovementsZScore

inTraining <- createDataPartition(spotiNormalizado$popularity, times=1,p = .80, list = FALSE)
training <- spotiNormalizado[ inTraining,]
testing  <- spotiNormalizado[-inTraining,]

training2 <- spotifyNormNoMovements[ inTraining,]
testing2 <- spotifyNormNoMovements [-inTraining,]

fitControl <- trainControl(
  method = "cv",  # Cross-validation
  number = 10,     # Número de folds
)
```


```{r, eval=FALSE}
modelo <- train(
  popularity ~ ., 
  data = training,
  method = "mlpKerasDropout",  # Puedes ajustar esto según el nombre del método que estás utilizando
  trControl = fitControl,
  tuneGrid = gridDropOut,
  epochs = 25
)


saveRDS(object = modelo, file = "mlpNorm2DropoutClasificacion.rds")
```

```{r}
mlpNorm2DropoutClasificacion = readRDS("mlpDropout/Clasificacion/mlpNorm2DropoutClasificacion.rds")
mlpNorm2DropoutClasificacion = calcular_metricas(mlpNorm2DropoutClasificacion, testing)
```

```{r, eval=FALSE}
modelo <- train(
  popularity ~ ., 
  data = training2,
  method = "mlpKerasDropout", 
  trControl = fitControl,
  tuneGrid = gridDropOut,
  epochs = 25
)

saveRDS(object = modelo, file = "mlpNormNoMov2DropoutClasificacion.rds")
```

```{r}
mlpNormNoMov2DropoutClasificacion = readRDS("mlpDropout/Clasificacion/mlpNormNoMov2DropoutClasificacion.rds")
mlpNormNoMov2DropoutClasificacion = calcular_metricas(mlpNormNoMov2DropoutClasificacion, testing2)
```

```{r, eval=FALSE}
# Entrenar el modelo utilizando train()
modelo <- train(
  popularity ~ ., 
  data = training,
  method = "mlpKerasDecay",  
  trControl = fitControl,
  tuneGrid = gridDecay,
  epochs = 25
)

saveRDS(object = modelo, file = "mlpNorm2DecayClasificacion.rds")
```

```{r}
mlpNorm2DecayClasificacion = readRDS("mlpDecay/Clasificacion/mlpNorm2DecayClasificacion.rds")
mlpNorm2DecayClasificacion = calcular_metricas(mlpNorm2DecayClasificacion, testing)
```

```{r, eval=FALSE}
# Entrenar el modelo utilizando train()
modelo <- train(
  popularity ~ ., 
  data = training2,
  method = "mlpKerasDecay",  # Puedes ajustar esto según el nombre del método que estás utilizando
  trControl = fitControl,
  tuneGrid = gridDecay,
  epochs = 25
)

saveRDS(object = modelo, file = "mlpNormNoMov2DecayClasificacion.rds")
```

```{r}
mlpNormNoMov2DecayClasificacion = readRDS("mlpDecay/Clasificacion/mlpNormNoMov2DecayClasificacion.rds")
mlpNormNoMov2DecayClasificacion = calcular_metricas(mlpNormNoMov2DecayClasificacion, testing2)
```

Finalmente tenemos ya calculadas todas las matrices de confusión, aunque no las vamos a plotear todas, solo la mejor.

```{r}
mlpNorm1DropoutClasificacion
```
Esta tambien nos da unos resultados relativamente altos aunque no tanto como los de arboles.


## knn
Por otra parte, el mejor modelo de clasificación en clustering es el modelo `knnCla_model` que tiene una precisión global del 92.24. En el que el hiperparámetro k toma el valor 10.

```{r}
knnCla_model$bestTune
cmknnCla$overall
```



Esta precisión, aunque tambien es alta, no es tan alta como la de `rf_500tm`.  

# Eleccion de Modelo

Al contrario que anteriormente no vamos a plotear la diferencia en nuestros modelos de precisión. Solo vamos a mencionar cual escogemos: va a ser el modelo de RandomForest debido a su gran precisión a la hora de clasificar los datos frente a las redes neuronales, quizás con un grid más grande para redes neuronales habriamos conseguido mejores resultados.

```{r}
# Modelos preentrenados
modelo_rf <- rf_500t
modelo_mlp <- mlpfinal
modelo_knn <- knnCla_model

# Recopilar Resamples
conjunto_resamples <- resamples(list(RandomForest = modelo_rf, 
                                     MLP = modelo_mlp, 
                                     KNN = modelo_knn))

# Visualización y Comparación
trellis.par.set(caretTheme())
bwplot(conjunto_resamples, layout = c(nrow = 3, ncol = 1))  
# Ajusta el diseño según la cantidad de modelos y métricas

# Pruebas de Hipótesis
diferencias <- diff(conjunto_resamples)
resumen_diferencias <- summary(diferencias)
```

